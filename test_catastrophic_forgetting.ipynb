{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MNF5-qJ7B0Q3"
   },
   "source": [
    "# Testing Stable Baselines3 with gym-MiniGrid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AdM2FnEJB0Q8"
   },
   "source": [
    "## Basic Jupyter Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 319,
     "status": "ok",
     "timestamp": 1647123362972,
     "user": {
      "displayName": "I単igo",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "14378798962183195551"
     },
     "user_tz": -60
    },
    "id": "aycUmr6OB0Q8"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pprint import pprint\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ef0DdE0b4pLd"
   },
   "source": [
    "## Import Stable-Baselines3 and initial set-up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PLM4YYcL5rBt"
   },
   "source": [
    "Import libraries and classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "YgenDMtf4pLe"
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import time\n",
    "import io\n",
    "import os\n",
    "import glob\n",
    "import torch\n",
    "import base64\n",
    "import stable_baselines3\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from random import randint \n",
    "from stable_baselines3 import PPO, DQN, A2C\n",
    "from stable_baselines3.common.results_plotter import ts2xy, load_results\n",
    "from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnRewardThreshold\n",
    "from stable_baselines3.common.env_util import make_atari_env\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.utils import set_random_seed\n",
    "from stable_baselines3.common.vec_env import SubprocVecEnv, DummyVecEnv\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "import gym\n",
    "from gym import spaces\n",
    "from gym.wrappers import Monitor\n",
    "import gym_minigrid\n",
    "from gym_minigrid.wrappers import FlatObsWrapper, ImgObsWrapper, RGBImgPartialObsWrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define wrapper for CNN Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ImgRGBImgPartialObsWrapper(env):\n",
    "    return ImgObsWrapper(RGBImgPartialObsWrapper(env))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "28U_WEp25rBu"
   },
   "source": [
    "Define the video function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "d7eCH8Kf4pLf"
   },
   "outputs": [],
   "source": [
    "import base64\n",
    "import glob\n",
    "import io\n",
    "from IPython.display import HTML\n",
    "from IPython import display \n",
    "\n",
    "def show_video():\n",
    "    mp4list = glob.glob('video/*.mp4')\n",
    "    if len(mp4list) > 0:\n",
    "        mp4 = mp4list[0]\n",
    "        video = io.open(mp4, 'r+b').read()\n",
    "        encoded = base64.b64encode(video)\n",
    "        display.display(HTML(data='''<video alt=\"test\" autoplay \n",
    "                loop controls style=\"height: 400px;\">\n",
    "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
    "             </video>'''.format(encoded.decode('ascii'))))\n",
    "    else:\n",
    "        print(\"Could not find video\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KchGuXpd5rBv"
   },
   "source": [
    "Define the rendering wrappers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Gdhk3Oep4pLf"
   },
   "outputs": [],
   "source": [
    "from gym.wrappers import Monitor\n",
    "\n",
    "# Monitor is a gym wrapper, which helps easy rendering of videos of the wrapped environment.\n",
    "def wrap_env(env):\n",
    "    env = Monitor(env, './video', force=True)\n",
    "    return env\n",
    "\n",
    "def gen_wrapped_env(env_name):\n",
    "    return wrap_env(FlatObsWrapper(gym.make(env_name)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "executionInfo": {
     "elapsed": 900,
     "status": "ok",
     "timestamp": 1647083269049,
     "user": {
      "displayName": "I単igo",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "14378798962183195551"
     },
     "user_tz": -60
    },
    "id": "B21JwGYn5rBv",
    "outputId": "54dfa526-2621-4f91-df2b-e4ff7a447aad"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from random import randint\n",
    "#env_id = 'MiniGrid-Empty-16x16-v0'\n",
    "env_id = 'MiniGrid-DoorKey-6x6-v0'\n",
    "#env_id = 'MiniGrid-DistShift1-v0'\n",
    "#env_id ='MiniGrid-UnlockPickup-v0'\n",
    "#env_id = 'MiniGrid-RedBlueDoors-6x6-v0'\n",
    "#env_id = 'MiniGrid-LavaGapS6-v0'\n",
    "\n",
    "eval_env = gym.make(env_id)\n",
    "eval_env.seed(10000+randint(0, 10))\n",
    "eval_env.reset()\n",
    "before_img = eval_env.render('rgb_array')\n",
    "\n",
    "plt.imshow(before_img);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "umY09KJP5rCI"
   },
   "source": [
    "# Catastrophic Forgetting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yPq1XkeL5rCI"
   },
   "source": [
    "## Define the environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eYzbUoTS5rCI"
   },
   "outputs": [],
   "source": [
    "# By default, we use a DummyVecEnv as it is usually faster (cf doc)\n",
    "num_cpu = 16  # Number of processes to use\n",
    "\n",
    "env_id_1 = 'MiniGrid-DoorKey-6x6-v0'\n",
    "vec_env_1 = make_vec_env(env_id_1, n_envs=num_cpu, wrapper_class=FlatObsWrapper, seed=10000)\n",
    "\n",
    "env_id_2 = 'MiniGrid-LavaGapS6-v0'\n",
    "vec_env_2 = make_vec_env(env_id_2, n_envs=num_cpu, wrapper_class=FlatObsWrapper, seed=5000)\n",
    "\n",
    "env_id_3 = 'MiniGrid-RedBlueDoors-6x6-v0'\n",
    "vec_env_3 = make_vec_env(env_id_3, n_envs=num_cpu, wrapper_class=FlatObsWrapper, seed=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 197,
     "status": "ok",
     "timestamp": 1647085386489,
     "user": {
      "displayName": "I単igo",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "14378798962183195551"
     },
     "user_tz": -60
    },
    "id": "vT24ysNm79CE",
    "outputId": "8f710f58-57f4-44c7-aebf-d24c88c99da3"
   },
   "outputs": [],
   "source": [
    "from stable_baselines3.common.utils import get_device\n",
    "get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "94zAf0-I5rCI"
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.00005\n",
    "n_steps = 256\n",
    "batch_size = 16\n",
    "ent_coef = 0.001\n",
    "n_epochs = 4\n",
    "\n",
    "tensorboard_log = \"./tmp/log/\"\n",
    "os.makedirs(tensorboard_log, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SDhlDj-J5rCI"
   },
   "source": [
    "## Learn first environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9700,
     "status": "ok",
     "timestamp": 1646782989154,
     "user": {
      "displayName": "I単igo",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "14378798962183195551"
     },
     "user_tz": -60
    },
    "id": "fgNWgfba5rCI",
    "outputId": "553d07ab-f3db-4c45-e45a-b2624d78a477",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Reset the environment\n",
    "vec_env_1.reset()\n",
    "\n",
    "# create the model\n",
    "model = PPO('MlpPolicy',\n",
    "            env=vec_env_1,\n",
    "            learning_rate=learning_rate,\n",
    "            batch_size=batch_size,\n",
    "            ent_coef=ent_coef,\n",
    "            n_epochs=n_epochs,\n",
    "            n_steps=n_steps,\n",
    "            tensorboard_log=tensorboard_log,\n",
    "            verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a75Hyh4n5rCJ"
   },
   "outputs": [],
   "source": [
    "log_dir = \"./tmp/gym/\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "tb_log_name = 'task01-doorkey-6x6'\n",
    "\n",
    "# Create eval environment\n",
    "env = gym.make(env_id_1)\n",
    "env = FlatObsWrapper(env)\n",
    "eval_env = stable_baselines3.common.monitor.Monitor(env, log_dir)\n",
    "# Reset the environment\n",
    "eval_env.reset();\n",
    "\n",
    "# Stop training when the model reaches the reward threshold\n",
    "callback_on_best = StopTrainingOnRewardThreshold(reward_threshold=0.92, verbose=1)\n",
    "eval_callback = EvalCallback(eval_env, log_path=log_dir, callback_on_new_best=callback_on_best, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cBedyr7x5rCJ",
    "outputId": "77956ad4-b758-4779-c8d2-3267555ba0b6",
    "tags": []
   },
   "outputs": [],
   "source": [
    "total_timesteps = 1000000\n",
    "log_interval = 10\n",
    "\n",
    "model.learn(total_timesteps=total_timesteps,\n",
    "            log_interval=log_interval,\n",
    "            tb_log_name = tb_log_name,\n",
    "            callback=eval_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DO-9hK8frbSj",
    "outputId": "772b6bd5-3a8e-40a9-bb47-a1bf792b7fdb"
   },
   "outputs": [],
   "source": [
    "print(tb_log_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4qWDyB355rCJ"
   },
   "outputs": [],
   "source": [
    "# Save the agent\n",
    "model.save(tb_log_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XtIhYAxd5rCJ"
   },
   "outputs": [],
   "source": [
    "#model = PPO.load(path=tb_log_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iz6ij9Nt5rCJ",
    "outputId": "85503a8d-7a79-493b-d06f-bd9ccbb0dc83"
   },
   "outputs": [],
   "source": [
    "# We create a separate environment for evaluation\n",
    "eval_env = FlatObsWrapper(gym.make(env_id_1))\n",
    "\n",
    "# Random Agent, before training\n",
    "mean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=20)\n",
    "print(f'Mean reward: {mean_reward} +/- {std_reward:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3qj5tsLD5rCK",
    "outputId": "52617138-a686-49a7-c489-4dc65b5caad7"
   },
   "outputs": [],
   "source": [
    "env_id = env_id_1\n",
    "test_env = gen_wrapped_env(env_id)\n",
    "# generate a random initialization for the environment\n",
    "\n",
    "test_env.seed(randint(1, num_cpu))\n",
    "observation = test_env.reset()\n",
    "\n",
    "done = False\n",
    "episode_reward = 0\n",
    "episode_length = 0\n",
    "\n",
    "while not done:\n",
    "  #test_env.render()\n",
    "  action, states = model.predict(observation, deterministic=False)\n",
    "  observation, reward, done, info = test_env.step(action)\n",
    "  episode_reward += reward\n",
    "  episode_length += 1\n",
    "\n",
    "print('Total reward:', episode_reward)\n",
    "print('Total length:', episode_length)\n",
    "\n",
    "test_env.close()\n",
    "show_video()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n0sfVEmCrbSl"
   },
   "source": [
    "## Learn second environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IhR_CogRrbSn"
   },
   "outputs": [],
   "source": [
    "#model = PPO.load(path=tb_log_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6X50nqSArbSl"
   },
   "outputs": [],
   "source": [
    "model.set_env(vec_env_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mLksPvHErbSl"
   },
   "outputs": [],
   "source": [
    "log_dir = \"./tmp/gym/\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "tb_log_name = 'task02-lava'\n",
    "\n",
    "# Create eval environment\n",
    "env = gym.make(env_id_2)\n",
    "env = FlatObsWrapper(env)\n",
    "eval_env = stable_baselines3.common.monitor.Monitor(env, log_dir)\n",
    "# Reset the environment\n",
    "eval_env.reset();\n",
    "\n",
    "# Stop training when the model reaches the reward threshold\n",
    "callback_on_best = StopTrainingOnRewardThreshold(reward_threshold=0.92, verbose=1)\n",
    "eval_callback = EvalCallback(eval_env, log_path=log_dir, callback_on_new_best=callback_on_best, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AaydIunmrbSm",
    "outputId": "5af05be1-5ff5-4e61-8059-df4c46ab4bab"
   },
   "outputs": [],
   "source": [
    "# number of timesteps to add\n",
    "total_timesteps = 5000000\n",
    "log_interval = 10\n",
    "\n",
    "# resume training using same tensorboard run without resetting the timesteps\n",
    "model.learn(total_timesteps=total_timesteps,\n",
    "            log_interval=log_interval,\n",
    "            tb_log_name=tb_log_name,\n",
    "            reset_num_timesteps=True,\n",
    "            callback=eval_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GpPwz9fGrbSm",
    "outputId": "73d281b0-a032-4592-b2c8-ae322d5095e2"
   },
   "outputs": [],
   "source": [
    "print(tb_log_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "krj7XuahrbSm"
   },
   "outputs": [],
   "source": [
    "# Save the agent\n",
    "model.save(tb_log_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HBowudtCrbSn",
    "outputId": "f769b0b6-742d-467d-e3fc-44b39fc98604"
   },
   "outputs": [],
   "source": [
    "# We create a separate environment for evaluation\n",
    "eval_env = FlatObsWrapper(gym.make(env_id_2))\n",
    "eval_env.seed(10000+randint(0, 10))\n",
    "eval_env.reset()\n",
    "# Random Agent, before training\n",
    "mean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=20)\n",
    "print(f'Mean reward: {mean_reward} +/- {std_reward:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FpW4ondMrbSo",
    "outputId": "60ea5f5b-2097-42cb-e0d4-c1bd04b9df36"
   },
   "outputs": [],
   "source": [
    "test_env = gen_wrapped_env(env_id_2)\n",
    "observation = test_env.reset()\n",
    "\n",
    "done = False\n",
    "episode_reward = 0\n",
    "episode_length = 0\n",
    "\n",
    "while not done:\n",
    "  #test_env.render()\n",
    "  action, states = model.predict(observation, deterministic=False)\n",
    "  observation, reward, done, info = test_env.step(action)\n",
    "  episode_reward += reward\n",
    "  episode_length += 1\n",
    "\n",
    "print('Total reward:', episode_reward)\n",
    "print('Total length:', episode_length)\n",
    "\n",
    "test_env.close()\n",
    "show_video()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-psMgyOBrbSo",
    "outputId": "1f570f75-744e-47e2-cfb3-d8654ee4c541"
   },
   "outputs": [],
   "source": [
    "# We create a separate environment for evaluation\n",
    "eval_env = FlatObsWrapper(gym.make(env_id_1))\n",
    "eval_env.seed(10000+randint(0, 10))\n",
    "eval_env.reset()\n",
    "# Random Agent, before training\n",
    "mean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=20)\n",
    "print(f'Mean reward: {mean_reward} +/- {std_reward:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s25OB5OKrbSo",
    "outputId": "b115e8ea-ecd2-4eeb-dc61-94fd59819f7a"
   },
   "outputs": [],
   "source": [
    "test_env = gen_wrapped_env(env_id_1)\n",
    "test_env.seed(10000+randint(0, 10))\n",
    "observation = test_env.reset()\n",
    "\n",
    "done = False\n",
    "episode_reward = 0\n",
    "episode_length = 0\n",
    "\n",
    "while not done:\n",
    "  #test_env.render()\n",
    "  action, states = model.predict(observation, deterministic=False)\n",
    "  observation, reward, done, info = test_env.step(action)\n",
    "  episode_reward += reward\n",
    "  episode_length += 1\n",
    "\n",
    "print('Total reward:', episode_reward)\n",
    "print('Total length:', episode_length)\n",
    "\n",
    "test_env.close()\n",
    "show_video()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RFTcnLV7rbSp"
   },
   "source": [
    "# Test training with CNN Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By default, we use a DummyVecEnv as it is usually faster (cf doc)\n",
    "num_cpu = 16  # Number of processes to use\n",
    "\n",
    "env_id_1 = 'MiniGrid-DoorKey-6x6-v0'\n",
    "vec_env_1 = make_vec_env(env_id_1, n_envs=num_cpu, wrapper_class=ImgRGBImgPartialObsWrapper, seed=10000)\n",
    "\n",
    "env_id_2 = 'MiniGrid-LavaGapS6-v0'\n",
    "vec_env_2 = make_vec_env(env_id_2, n_envs=num_cpu, wrapper_class=ImgRGBImgPartialObsWrapper, seed=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test which device is in use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from stable_baselines3.common.utils import get_device\n",
    "get_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test wrapper output shape:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation: [[[[55 55 55]\n",
      "   [33 33 33]\n",
      "   [33 33 33]\n",
      "   ...\n",
      "   [33 33 33]\n",
      "   [33 33 33]\n",
      "   [33 33 33]]\n",
      "\n",
      "  [[33 33 33]\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]\n",
      "   ...\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]]\n",
      "\n",
      "  [[33 33 33]\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]\n",
      "   ...\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[99 99 99]\n",
      "   [76 76 76]\n",
      "   [76 76 76]\n",
      "   ...\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]]\n",
      "\n",
      "  [[99 99 99]\n",
      "   [76 76 76]\n",
      "   [76 76 76]\n",
      "   ...\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]]\n",
      "\n",
      "  [[99 99 99]\n",
      "   [76 76 76]\n",
      "   [76 76 76]\n",
      "   ...\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]]]\n",
      "\n",
      "\n",
      " [[[55 55 55]\n",
      "   [33 33 33]\n",
      "   [33 33 33]\n",
      "   ...\n",
      "   [33 33 33]\n",
      "   [33 33 33]\n",
      "   [33 33 33]]\n",
      "\n",
      "  [[33 33 33]\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]\n",
      "   ...\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]]\n",
      "\n",
      "  [[33 33 33]\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]\n",
      "   ...\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[33 33 33]\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]\n",
      "   ...\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]]\n",
      "\n",
      "  [[33 33 33]\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]\n",
      "   ...\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]]\n",
      "\n",
      "  [[33 33 33]\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]\n",
      "   ...\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]]]\n",
      "\n",
      "\n",
      " [[[55 55 55]\n",
      "   [33 33 33]\n",
      "   [33 33 33]\n",
      "   ...\n",
      "   [33 33 33]\n",
      "   [33 33 33]\n",
      "   [33 33 33]]\n",
      "\n",
      "  [[33 33 33]\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]\n",
      "   ...\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]]\n",
      "\n",
      "  [[33 33 33]\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]\n",
      "   ...\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[99 99 99]\n",
      "   [76 76 76]\n",
      "   [76 76 76]\n",
      "   ...\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]]\n",
      "\n",
      "  [[99 99 99]\n",
      "   [76 76 76]\n",
      "   [76 76 76]\n",
      "   ...\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]]\n",
      "\n",
      "  [[99 99 99]\n",
      "   [76 76 76]\n",
      "   [76 76 76]\n",
      "   ...\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[55 55 55]\n",
      "   [33 33 33]\n",
      "   [33 33 33]\n",
      "   ...\n",
      "   [33 33 33]\n",
      "   [33 33 33]\n",
      "   [33 33 33]]\n",
      "\n",
      "  [[33 33 33]\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]\n",
      "   ...\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]]\n",
      "\n",
      "  [[33 33 33]\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]\n",
      "   ...\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[33 33 33]\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]\n",
      "   ...\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]]\n",
      "\n",
      "  [[33 33 33]\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]\n",
      "   ...\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]]\n",
      "\n",
      "  [[33 33 33]\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]\n",
      "   ...\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]]]\n",
      "\n",
      "\n",
      " [[[55 55 55]\n",
      "   [33 33 33]\n",
      "   [33 33 33]\n",
      "   ...\n",
      "   [33 33 33]\n",
      "   [33 33 33]\n",
      "   [33 33 33]]\n",
      "\n",
      "  [[33 33 33]\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]\n",
      "   ...\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]]\n",
      "\n",
      "  [[33 33 33]\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]\n",
      "   ...\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[99 99 99]\n",
      "   [76 76 76]\n",
      "   [76 76 76]\n",
      "   ...\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]]\n",
      "\n",
      "  [[99 99 99]\n",
      "   [76 76 76]\n",
      "   [76 76 76]\n",
      "   ...\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]]\n",
      "\n",
      "  [[99 99 99]\n",
      "   [76 76 76]\n",
      "   [76 76 76]\n",
      "   ...\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]]]\n",
      "\n",
      "\n",
      " [[[55 55 55]\n",
      "   [33 33 33]\n",
      "   [33 33 33]\n",
      "   ...\n",
      "   [33 33 33]\n",
      "   [33 33 33]\n",
      "   [33 33 33]]\n",
      "\n",
      "  [[33 33 33]\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]\n",
      "   ...\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]]\n",
      "\n",
      "  [[33 33 33]\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]\n",
      "   ...\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[33 33 33]\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]\n",
      "   ...\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]]\n",
      "\n",
      "  [[33 33 33]\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]\n",
      "   ...\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]]\n",
      "\n",
      "  [[33 33 33]\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]\n",
      "   ...\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]]]] , Observation Shape:  (16, 56, 56, 3)\n"
     ]
    }
   ],
   "source": [
    "obs = vec_env_1.reset()\n",
    "print('Observation:', obs, ', Observation Shape: ', obs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.00005\n",
    "n_steps = 256\n",
    "batch_size = 16\n",
    "ent_coef = 0.001\n",
    "n_epochs = 4\n",
    "\n",
    "tensorboard_log = \"./tmp/log/\"\n",
    "os.makedirs(tensorboard_log, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learn first environment with CnnPolicy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env in a VecTransposeImage.\n"
     ]
    }
   ],
   "source": [
    "# Reset the environment\n",
    "vec_env_1.reset()\n",
    "\n",
    "# create the model\n",
    "model = PPO('CnnPolicy',\n",
    "            env=vec_env_1,\n",
    "            learning_rate=learning_rate,\n",
    "            batch_size=batch_size,\n",
    "            ent_coef=ent_coef,\n",
    "            n_epochs=n_epochs,\n",
    "            n_steps=n_steps,\n",
    "            tensorboard_log=tensorboard_log,\n",
    "            verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"./tmp/gym/\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "tb_log_name = 'task01-doorkey-6x6_cnn'\n",
    "\n",
    "# Create eval environment\n",
    "env = gym.make(env_id_1)\n",
    "env = ImgRGBImgPartialObsWrapper(env)\n",
    "eval_env = stable_baselines3.common.monitor.Monitor(env, log_dir)\n",
    "# Reset the environment\n",
    "eval_env.reset();\n",
    "\n",
    "# Stop training when the model reaches the reward threshold\n",
    "callback_on_best = StopTrainingOnRewardThreshold(reward_threshold=0.92, verbose=1)\n",
    "eval_callback = EvalCallback(eval_env, log_path=log_dir, callback_on_new_best=callback_on_best, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to ./tmp/log/task01-doorkey-6x6_cnn_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/inigo/Documents/cursos/Data Science/master_viu/work/minigrid/stable-baselines3/stable_baselines3/common/callbacks.py:345: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.vec_transpose.VecTransposeImage object at 0x14597d750> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x14548a2d0>\n",
      "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 359         |\n",
      "|    ep_rew_mean          | 0.00425     |\n",
      "| time/                   |             |\n",
      "|    fps                  | 310         |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 131         |\n",
      "|    total_timesteps      | 40960       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007972017 |\n",
      "|    clip_fraction        | 0.0811      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.85       |\n",
      "|    explained_variance   | -1.05       |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | 0.0155      |\n",
      "|    n_updates            | 36          |\n",
      "|    policy_gradient_loss | -0.00603    |\n",
      "|    value_loss           | 0.000258    |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 360        |\n",
      "|    ep_rew_mean          | 0          |\n",
      "| time/                   |            |\n",
      "|    fps                  | 298        |\n",
      "|    iterations           | 20         |\n",
      "|    time_elapsed         | 274        |\n",
      "|    total_timesteps      | 81920      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00891549 |\n",
      "|    clip_fraction        | 0.0676     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.82      |\n",
      "|    explained_variance   | -2.16      |\n",
      "|    learning_rate        | 5e-05      |\n",
      "|    loss                 | -0.0518    |\n",
      "|    n_updates            | 76         |\n",
      "|    policy_gradient_loss | -0.00594   |\n",
      "|    value_loss           | 0.000172   |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 360         |\n",
      "|    ep_rew_mean          | 0           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 283         |\n",
      "|    iterations           | 30          |\n",
      "|    time_elapsed         | 433         |\n",
      "|    total_timesteps      | 122880      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008203281 |\n",
      "|    clip_fraction        | 0.0737      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.73       |\n",
      "|    explained_variance   | -3.41       |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | 0.0192      |\n",
      "|    n_updates            | 116         |\n",
      "|    policy_gradient_loss | -0.00322    |\n",
      "|    value_loss           | 0.000138    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=160000, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 360.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 360         |\n",
      "|    mean_reward          | 0           |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 160000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014236996 |\n",
      "|    clip_fraction        | 0.1         |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.72       |\n",
      "|    explained_variance   | -3.65       |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0435     |\n",
      "|    n_updates            | 156         |\n",
      "|    policy_gradient_loss | -0.00458    |\n",
      "|    value_loss           | 0.000114    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 360      |\n",
      "|    ep_rew_mean     | 0.00138  |\n",
      "| time/              |          |\n",
      "|    fps             | 277      |\n",
      "|    iterations      | 40       |\n",
      "|    time_elapsed    | 590      |\n",
      "|    total_timesteps | 163840   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 358         |\n",
      "|    ep_rew_mean          | 0.00675     |\n",
      "| time/                   |             |\n",
      "|    fps                  | 274         |\n",
      "|    iterations           | 50          |\n",
      "|    time_elapsed         | 745         |\n",
      "|    total_timesteps      | 204800      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008412058 |\n",
      "|    clip_fraction        | 0.068       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.74       |\n",
      "|    explained_variance   | -0.247      |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0539     |\n",
      "|    n_updates            | 196         |\n",
      "|    policy_gradient_loss | -0.00591    |\n",
      "|    value_loss           | 0.000573    |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/mj/7zvl797s38qgzlm091ctt_gw0000gn/T/ipykernel_61894/3677581745.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m             \u001b[0mlog_interval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlog_interval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m             \u001b[0mtb_log_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtb_log_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m             callback=eval_callback)\n\u001b[0m",
      "\u001b[0;32m~/Documents/cursos/Data Science/master_viu/work/minigrid/stable-baselines3/stable_baselines3/ppo/ppo.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps)\u001b[0m\n\u001b[1;32m    311\u001b[0m             \u001b[0mtb_log_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtb_log_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m             \u001b[0meval_log_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_log_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 313\u001b[0;31m             \u001b[0mreset_num_timesteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreset_num_timesteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    314\u001b[0m         )\n",
      "\u001b[0;32m~/Documents/cursos/Data Science/master_viu/work/minigrid/stable-baselines3/stable_baselines3/common/on_policy_algorithm.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps)\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_timesteps\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0mcontinue_training\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect_rollouts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrollout_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_rollout_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcontinue_training\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/cursos/Data Science/master_viu/work/minigrid/stable-baselines3/stable_baselines3/common/on_policy_algorithm.py\u001b[0m in \u001b[0;36mcollect_rollouts\u001b[0;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[1;32m    176\u001b[0m                 \u001b[0mclipped_actions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhigh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m             \u001b[0mnew_obs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdones\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclipped_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_timesteps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_envs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/cursos/Data Science/master_viu/work/minigrid/stable-baselines3/stable_baselines3/common/vec_env/base_vec_env.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    160\u001b[0m         \"\"\"\n\u001b[1;32m    161\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/cursos/Data Science/master_viu/work/minigrid/stable-baselines3/stable_baselines3/common/vec_env/vec_transpose.py\u001b[0m in \u001b[0;36mstep_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mVecEnvStepReturn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m         \u001b[0mobservations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdones\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvenv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;31m# Transpose the terminal observations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/cursos/Data Science/master_viu/work/minigrid/stable-baselines3/stable_baselines3/common/vec_env/dummy_vec_env.py\u001b[0m in \u001b[0;36mstep_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0menv_idx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_envs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             obs, self.buf_rews[env_idx], self.buf_dones[env_idx], self.buf_infos[env_idx] = self.envs[env_idx].step(\n\u001b[0;32m---> 44\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0menv_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m             )\n\u001b[1;32m     46\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuf_dones\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0menv_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/minigrid-k7XHdCuO/lib/python3.7/site-packages/gym/core.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    321\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 323\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    324\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/minigrid-k7XHdCuO/lib/python3.7/site-packages/gym/core.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    321\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 323\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    324\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/cursos/Data Science/master_viu/work/minigrid/stable-baselines3/stable_baselines3/common/monitor.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mneeds_reset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Tried to step environment that needs reset\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/minigrid-k7XHdCuO/lib/python3.7/site-packages/gym/wrappers/order_enforcing.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_reset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Cannot call env.step() before calling reset()\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/cursos/Data Science/master_viu/work/minigrid/gym-minigrid/gym_minigrid/minigrid.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m   1159\u001b[0m             \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1161\u001b[0;31m         \u001b[0mobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen_obs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1163\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/cursos/Data Science/master_viu/work/minigrid/gym-minigrid/gym_minigrid/minigrid.py\u001b[0m in \u001b[0;36mgen_obs\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1204\u001b[0m         \u001b[0;31m# Encode the partially observable view into a numpy array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1205\u001b[0;31m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvis_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1207\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'mission'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"environments must define a textual mission string\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/cursos/Data Science/master_viu/work/minigrid/gym-minigrid/gym_minigrid/minigrid.py\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, vis_mask)\u001b[0m\n\u001b[1;32m    544\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    545\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mvis_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 546\u001b[0;31m                     \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    547\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "total_timesteps = 1000000\n",
    "log_interval = 10\n",
    "\n",
    "model.learn(total_timesteps=total_timesteps,\n",
    "            log_interval=log_interval,\n",
    "            tb_log_name = tb_log_name,\n",
    "            callback=eval_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the agent\n",
    "model.save(tb_log_name)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "test_minigrid_sb3_curriculum.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "51a1b245ec7fc72d2061587b663ba1f584a95fe24cf1a61a943e144e8f966db4"
  },
  "kernelspec": {
   "display_name": "minigrid",
   "language": "python",
   "name": "minigrid"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
