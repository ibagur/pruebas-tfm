{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MNF5-qJ7B0Q3"
   },
   "source": [
    "# Testing Stable Baselines3 with gym-MiniGrid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AdM2FnEJB0Q8"
   },
   "source": [
    "## Basic Jupyter Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 319,
     "status": "ok",
     "timestamp": 1647123362972,
     "user": {
      "displayName": "I単igo",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "14378798962183195551"
     },
     "user_tz": -60
    },
    "id": "aycUmr6OB0Q8"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pprint import pprint\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ef0DdE0b4pLd"
   },
   "source": [
    "## Import Stable-Baselines3 and initial set-up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PLM4YYcL5rBt"
   },
   "source": [
    "Import libraries and classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "YgenDMtf4pLe"
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import time\n",
    "import io\n",
    "import os\n",
    "import glob\n",
    "import torch\n",
    "import base64\n",
    "import stable_baselines3\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from random import randint \n",
    "from stable_baselines3 import PPO, DQN, A2C\n",
    "from stable_baselines3.common.results_plotter import ts2xy, load_results\n",
    "from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnRewardThreshold\n",
    "from stable_baselines3.common.env_util import make_atari_env\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.utils import set_random_seed\n",
    "from stable_baselines3.common.vec_env import SubprocVecEnv, DummyVecEnv\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "import gym\n",
    "from gym import spaces\n",
    "from gym.wrappers import Monitor\n",
    "import gym_minigrid\n",
    "from gym_minigrid.wrappers import FlatObsWrapper, ImgObsWrapper, RGBImgPartialObsWrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define wrapper for CNN Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ImgRGBImgPartialObsWrapper(env):\n",
    "    return ImgObsWrapper(RGBImgPartialObsWrapper(env))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "28U_WEp25rBu"
   },
   "source": [
    "Define the video function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "d7eCH8Kf4pLf"
   },
   "outputs": [],
   "source": [
    "import base64\n",
    "import glob\n",
    "import io\n",
    "from IPython.display import HTML\n",
    "from IPython import display \n",
    "\n",
    "def show_video():\n",
    "    mp4list = glob.glob('video/*.mp4')\n",
    "    if len(mp4list) > 0:\n",
    "        mp4 = mp4list[0]\n",
    "        video = io.open(mp4, 'r+b').read()\n",
    "        encoded = base64.b64encode(video)\n",
    "        display.display(HTML(data='''<video alt=\"test\" autoplay \n",
    "                loop controls style=\"height: 400px;\">\n",
    "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
    "             </video>'''.format(encoded.decode('ascii'))))\n",
    "    else:\n",
    "        print(\"Could not find video\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KchGuXpd5rBv"
   },
   "source": [
    "Define the rendering wrappers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Gdhk3Oep4pLf"
   },
   "outputs": [],
   "source": [
    "from gym.wrappers import Monitor\n",
    "\n",
    "# Monitor is a gym wrapper, which helps easy rendering of videos of the wrapped environment.\n",
    "def wrap_env(env):\n",
    "    env = Monitor(env, './video', force=True)\n",
    "    return env\n",
    "\n",
    "def gen_wrapped_env(env_name):\n",
    "    return wrap_env(FlatObsWrapper(gym.make(env_name)))\n",
    "\n",
    "def gen_wrapped_env_cnn(env_name):\n",
    "    return wrap_env(ImgObsWrapper(RGBImgPartialObsWrapper(gym.make(env_name))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "executionInfo": {
     "elapsed": 900,
     "status": "ok",
     "timestamp": 1647083269049,
     "user": {
      "displayName": "I単igo",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "14378798962183195551"
     },
     "user_tz": -60
    },
    "id": "B21JwGYn5rBv",
    "outputId": "54dfa526-2621-4f91-df2b-e4ff7a447aad"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from random import randint\n",
    "#env_id = 'MiniGrid-Empty-16x16-v0'\n",
    "env_id = 'MiniGrid-DoorKey-6x6-v0'\n",
    "#env_id = 'MiniGrid-DistShift1-v0'\n",
    "#env_id ='MiniGrid-UnlockPickup-v0'\n",
    "#env_id = 'MiniGrid-RedBlueDoors-6x6-v0'\n",
    "#env_id = 'MiniGrid-LavaGapS6-v0'\n",
    "\n",
    "eval_env = gym.make(env_id)\n",
    "eval_env.seed(10000+randint(0, 10))\n",
    "eval_env.reset()\n",
    "before_img = eval_env.render('rgb_array')\n",
    "\n",
    "plt.imshow(before_img);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "umY09KJP5rCI"
   },
   "source": [
    "# Catastrophic Forgetting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yPq1XkeL5rCI"
   },
   "source": [
    "## Define the environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eYzbUoTS5rCI"
   },
   "outputs": [],
   "source": [
    "# By default, we use a DummyVecEnv as it is usually faster (cf doc)\n",
    "num_cpu = 16  # Number of processes to use\n",
    "\n",
    "env_id_1 = 'MiniGrid-DoorKey-6x6-v0'\n",
    "vec_env_1 = make_vec_env(env_id_1, n_envs=num_cpu, wrapper_class=FlatObsWrapper, seed=10000)\n",
    "\n",
    "env_id_2 = 'MiniGrid-LavaGapS6-v0'\n",
    "vec_env_2 = make_vec_env(env_id_2, n_envs=num_cpu, wrapper_class=FlatObsWrapper, seed=5000)\n",
    "\n",
    "env_id_3 = 'MiniGrid-RedBlueDoors-6x6-v0'\n",
    "vec_env_3 = make_vec_env(env_id_3, n_envs=num_cpu, wrapper_class=FlatObsWrapper, seed=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 197,
     "status": "ok",
     "timestamp": 1647085386489,
     "user": {
      "displayName": "I単igo",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "14378798962183195551"
     },
     "user_tz": -60
    },
    "id": "vT24ysNm79CE",
    "outputId": "8f710f58-57f4-44c7-aebf-d24c88c99da3"
   },
   "outputs": [],
   "source": [
    "from stable_baselines3.common.utils import get_device\n",
    "get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "94zAf0-I5rCI"
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.00005\n",
    "n_steps = 256\n",
    "batch_size = 16\n",
    "ent_coef = 0.001\n",
    "n_epochs = 4\n",
    "\n",
    "tensorboard_log = \"./tmp/log/\"\n",
    "os.makedirs(tensorboard_log, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SDhlDj-J5rCI"
   },
   "source": [
    "## Learn first environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9700,
     "status": "ok",
     "timestamp": 1646782989154,
     "user": {
      "displayName": "I単igo",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "14378798962183195551"
     },
     "user_tz": -60
    },
    "id": "fgNWgfba5rCI",
    "outputId": "553d07ab-f3db-4c45-e45a-b2624d78a477",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Reset the environment\n",
    "vec_env_1.reset()\n",
    "\n",
    "# create the model\n",
    "model = PPO('MlpPolicy',\n",
    "            env=vec_env_1,\n",
    "            learning_rate=learning_rate,\n",
    "            batch_size=batch_size,\n",
    "            ent_coef=ent_coef,\n",
    "            n_epochs=n_epochs,\n",
    "            n_steps=n_steps,\n",
    "            tensorboard_log=tensorboard_log,\n",
    "            verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a75Hyh4n5rCJ"
   },
   "outputs": [],
   "source": [
    "log_dir = \"./tmp/gym/\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "tb_log_name = 'task01-doorkey-6x6'\n",
    "\n",
    "# Create eval environment\n",
    "env = gym.make(env_id_1)\n",
    "env = FlatObsWrapper(env)\n",
    "eval_env = stable_baselines3.common.monitor.Monitor(env, log_dir)\n",
    "# Reset the environment\n",
    "eval_env.reset();\n",
    "\n",
    "# Stop training when the model reaches the reward threshold\n",
    "callback_on_best = StopTrainingOnRewardThreshold(reward_threshold=0.92, verbose=1)\n",
    "eval_callback = EvalCallback(eval_env, log_path=log_dir, callback_on_new_best=callback_on_best, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cBedyr7x5rCJ",
    "outputId": "77956ad4-b758-4779-c8d2-3267555ba0b6",
    "tags": []
   },
   "outputs": [],
   "source": [
    "total_timesteps = 1000000\n",
    "log_interval = 10\n",
    "\n",
    "model.learn(total_timesteps=total_timesteps,\n",
    "            log_interval=log_interval,\n",
    "            tb_log_name = tb_log_name,\n",
    "            callback=eval_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DO-9hK8frbSj",
    "outputId": "772b6bd5-3a8e-40a9-bb47-a1bf792b7fdb"
   },
   "outputs": [],
   "source": [
    "print(tb_log_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4qWDyB355rCJ"
   },
   "outputs": [],
   "source": [
    "# Save the agent\n",
    "model.save(tb_log_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XtIhYAxd5rCJ"
   },
   "outputs": [],
   "source": [
    "#model = PPO.load(path=tb_log_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iz6ij9Nt5rCJ",
    "outputId": "85503a8d-7a79-493b-d06f-bd9ccbb0dc83"
   },
   "outputs": [],
   "source": [
    "# We create a separate environment for evaluation\n",
    "eval_env = FlatObsWrapper(gym.make(env_id_1))\n",
    "\n",
    "# Random Agent, before training\n",
    "mean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=20)\n",
    "print(f'Mean reward: {mean_reward} +/- {std_reward:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3qj5tsLD5rCK",
    "outputId": "52617138-a686-49a7-c489-4dc65b5caad7"
   },
   "outputs": [],
   "source": [
    "env_id = env_id_1\n",
    "test_env = gen_wrapped_env(env_id)\n",
    "# generate a random initialization for the environment\n",
    "\n",
    "test_env.seed(randint(1, num_cpu))\n",
    "observation = test_env.reset()\n",
    "\n",
    "done = False\n",
    "episode_reward = 0\n",
    "episode_length = 0\n",
    "\n",
    "while not done:\n",
    "  #test_env.render()\n",
    "  action, states = model.predict(observation, deterministic=False)\n",
    "  observation, reward, done, info = test_env.step(action)\n",
    "  episode_reward += reward\n",
    "  episode_length += 1\n",
    "\n",
    "print('Total reward:', episode_reward)\n",
    "print('Total length:', episode_length)\n",
    "\n",
    "test_env.close()\n",
    "show_video()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n0sfVEmCrbSl"
   },
   "source": [
    "## Learn second environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IhR_CogRrbSn"
   },
   "outputs": [],
   "source": [
    "#model = PPO.load(path=tb_log_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6X50nqSArbSl"
   },
   "outputs": [],
   "source": [
    "model.set_env(vec_env_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mLksPvHErbSl"
   },
   "outputs": [],
   "source": [
    "log_dir = \"./tmp/gym/\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "tb_log_name = 'task02-lava'\n",
    "\n",
    "# Create eval environment\n",
    "env = gym.make(env_id_2)\n",
    "env = FlatObsWrapper(env)\n",
    "eval_env = stable_baselines3.common.monitor.Monitor(env, log_dir)\n",
    "# Reset the environment\n",
    "eval_env.reset();\n",
    "\n",
    "# Stop training when the model reaches the reward threshold\n",
    "callback_on_best = StopTrainingOnRewardThreshold(reward_threshold=0.92, verbose=1)\n",
    "eval_callback = EvalCallback(eval_env, log_path=log_dir, callback_on_new_best=callback_on_best, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AaydIunmrbSm",
    "outputId": "5af05be1-5ff5-4e61-8059-df4c46ab4bab"
   },
   "outputs": [],
   "source": [
    "# number of timesteps to add\n",
    "total_timesteps = 5000000\n",
    "log_interval = 10\n",
    "\n",
    "# resume training using same tensorboard run without resetting the timesteps\n",
    "model.learn(total_timesteps=total_timesteps,\n",
    "            log_interval=log_interval,\n",
    "            tb_log_name=tb_log_name,\n",
    "            reset_num_timesteps=True,\n",
    "            callback=eval_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GpPwz9fGrbSm",
    "outputId": "73d281b0-a032-4592-b2c8-ae322d5095e2"
   },
   "outputs": [],
   "source": [
    "print(tb_log_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "krj7XuahrbSm"
   },
   "outputs": [],
   "source": [
    "# Save the agent\n",
    "model.save(tb_log_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HBowudtCrbSn",
    "outputId": "f769b0b6-742d-467d-e3fc-44b39fc98604"
   },
   "outputs": [],
   "source": [
    "# We create a separate environment for evaluation\n",
    "eval_env = FlatObsWrapper(gym.make(env_id_2))\n",
    "eval_env.seed(10000+randint(0, 10))\n",
    "eval_env.reset()\n",
    "# Random Agent, before training\n",
    "mean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=20)\n",
    "print(f'Mean reward: {mean_reward} +/- {std_reward:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FpW4ondMrbSo",
    "outputId": "60ea5f5b-2097-42cb-e0d4-c1bd04b9df36"
   },
   "outputs": [],
   "source": [
    "test_env = gen_wrapped_env(env_id_2)\n",
    "observation = test_env.reset()\n",
    "\n",
    "done = False\n",
    "episode_reward = 0\n",
    "episode_length = 0\n",
    "\n",
    "while not done:\n",
    "  #test_env.render()\n",
    "  action, states = model.predict(observation, deterministic=False)\n",
    "  observation, reward, done, info = test_env.step(action)\n",
    "  episode_reward += reward\n",
    "  episode_length += 1\n",
    "\n",
    "print('Total reward:', episode_reward)\n",
    "print('Total length:', episode_length)\n",
    "\n",
    "test_env.close()\n",
    "show_video()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-psMgyOBrbSo",
    "outputId": "1f570f75-744e-47e2-cfb3-d8654ee4c541"
   },
   "outputs": [],
   "source": [
    "# We create a separate environment for evaluation\n",
    "eval_env = FlatObsWrapper(gym.make(env_id_1))\n",
    "eval_env.seed(10000+randint(0, 10))\n",
    "eval_env.reset()\n",
    "# Random Agent, before training\n",
    "mean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=20)\n",
    "print(f'Mean reward: {mean_reward} +/- {std_reward:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s25OB5OKrbSo",
    "outputId": "b115e8ea-ecd2-4eeb-dc61-94fd59819f7a"
   },
   "outputs": [],
   "source": [
    "test_env = gen_wrapped_env(env_id_1)\n",
    "test_env.seed(10000+randint(0, 10))\n",
    "observation = test_env.reset()\n",
    "\n",
    "done = False\n",
    "episode_reward = 0\n",
    "episode_length = 0\n",
    "\n",
    "while not done:\n",
    "  #test_env.render()\n",
    "  action, states = model.predict(observation, deterministic=False)\n",
    "  observation, reward, done, info = test_env.step(action)\n",
    "  episode_reward += reward\n",
    "  episode_length += 1\n",
    "\n",
    "print('Total reward:', episode_reward)\n",
    "print('Total length:', episode_length)\n",
    "\n",
    "test_env.close()\n",
    "show_video()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RFTcnLV7rbSp"
   },
   "source": [
    "# Test training with CNN Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By default, we use a DummyVecEnv as it is usually faster (cf doc)\n",
    "num_cpu = 16  # Number of processes to use\n",
    "\n",
    "env_id_1 = 'MiniGrid-DoorKey-6x6-v0'\n",
    "vec_env_1 = make_vec_env(env_id_1, n_envs=num_cpu, wrapper_class=ImgRGBImgPartialObsWrapper, seed=10000)\n",
    "\n",
    "env_id_2 = 'MiniGrid-LavaGapS6-v0'\n",
    "vec_env_2 = make_vec_env(env_id_2, n_envs=num_cpu, wrapper_class=ImgRGBImgPartialObsWrapper, seed=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test which device is in use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from stable_baselines3.common.utils import get_device\n",
    "get_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test wrapper output shape:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation: [[[[55 55 55]\n",
      "   [33 33 33]\n",
      "   [33 33 33]\n",
      "   ...\n",
      "   [33 33 33]\n",
      "   [33 33 33]\n",
      "   [33 33 33]]\n",
      "\n",
      "  [[33 33 33]\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]\n",
      "   ...\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]]\n",
      "\n",
      "  [[33 33 33]\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]\n",
      "   ...\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[99 99 99]\n",
      "   [76 76 76]\n",
      "   [76 76 76]\n",
      "   ...\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]]\n",
      "\n",
      "  [[99 99 99]\n",
      "   [76 76 76]\n",
      "   [76 76 76]\n",
      "   ...\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]]\n",
      "\n",
      "  [[99 99 99]\n",
      "   [76 76 76]\n",
      "   [76 76 76]\n",
      "   ...\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]]]\n",
      "\n",
      "\n",
      " [[[55 55 55]\n",
      "   [33 33 33]\n",
      "   [33 33 33]\n",
      "   ...\n",
      "   [33 33 33]\n",
      "   [33 33 33]\n",
      "   [33 33 33]]\n",
      "\n",
      "  [[33 33 33]\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]\n",
      "   ...\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]]\n",
      "\n",
      "  [[33 33 33]\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]\n",
      "   ...\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[33 33 33]\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]\n",
      "   ...\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]]\n",
      "\n",
      "  [[33 33 33]\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]\n",
      "   ...\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]]\n",
      "\n",
      "  [[33 33 33]\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]\n",
      "   ...\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]]]\n",
      "\n",
      "\n",
      " [[[55 55 55]\n",
      "   [33 33 33]\n",
      "   [33 33 33]\n",
      "   ...\n",
      "   [33 33 33]\n",
      "   [33 33 33]\n",
      "   [33 33 33]]\n",
      "\n",
      "  [[33 33 33]\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]\n",
      "   ...\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]]\n",
      "\n",
      "  [[33 33 33]\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]\n",
      "   ...\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[99 99 99]\n",
      "   [76 76 76]\n",
      "   [76 76 76]\n",
      "   ...\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]]\n",
      "\n",
      "  [[99 99 99]\n",
      "   [76 76 76]\n",
      "   [76 76 76]\n",
      "   ...\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]]\n",
      "\n",
      "  [[99 99 99]\n",
      "   [76 76 76]\n",
      "   [76 76 76]\n",
      "   ...\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[55 55 55]\n",
      "   [33 33 33]\n",
      "   [33 33 33]\n",
      "   ...\n",
      "   [33 33 33]\n",
      "   [33 33 33]\n",
      "   [33 33 33]]\n",
      "\n",
      "  [[33 33 33]\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]\n",
      "   ...\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]]\n",
      "\n",
      "  [[33 33 33]\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]\n",
      "   ...\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[33 33 33]\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]\n",
      "   ...\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]]\n",
      "\n",
      "  [[33 33 33]\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]\n",
      "   ...\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]]\n",
      "\n",
      "  [[33 33 33]\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]\n",
      "   ...\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]]]\n",
      "\n",
      "\n",
      " [[[55 55 55]\n",
      "   [33 33 33]\n",
      "   [33 33 33]\n",
      "   ...\n",
      "   [33 33 33]\n",
      "   [33 33 33]\n",
      "   [33 33 33]]\n",
      "\n",
      "  [[33 33 33]\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]\n",
      "   ...\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]]\n",
      "\n",
      "  [[33 33 33]\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]\n",
      "   ...\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[99 99 99]\n",
      "   [76 76 76]\n",
      "   [76 76 76]\n",
      "   ...\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]]\n",
      "\n",
      "  [[99 99 99]\n",
      "   [76 76 76]\n",
      "   [76 76 76]\n",
      "   ...\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]]\n",
      "\n",
      "  [[99 99 99]\n",
      "   [76 76 76]\n",
      "   [76 76 76]\n",
      "   ...\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]]]\n",
      "\n",
      "\n",
      " [[[55 55 55]\n",
      "   [33 33 33]\n",
      "   [33 33 33]\n",
      "   ...\n",
      "   [33 33 33]\n",
      "   [33 33 33]\n",
      "   [33 33 33]]\n",
      "\n",
      "  [[33 33 33]\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]\n",
      "   ...\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]]\n",
      "\n",
      "  [[33 33 33]\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]\n",
      "   ...\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[33 33 33]\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]\n",
      "   ...\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]]\n",
      "\n",
      "  [[33 33 33]\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]\n",
      "   ...\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]]\n",
      "\n",
      "  [[33 33 33]\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]\n",
      "   ...\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]]]] , Observation Shape:  (16, 56, 56, 3)\n"
     ]
    }
   ],
   "source": [
    "obs = vec_env_1.reset()\n",
    "print('Observation:', obs, ', Observation Shape: ', obs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.00005\n",
    "n_steps = 256\n",
    "batch_size = 16\n",
    "ent_coef = 0.001\n",
    "n_epochs = 4\n",
    "\n",
    "tensorboard_log = \"./tmp/log/\"\n",
    "os.makedirs(tensorboard_log, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learn first environment with CnnPolicy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env in a VecTransposeImage.\n"
     ]
    }
   ],
   "source": [
    "# Reset the environment\n",
    "vec_env_1.reset()\n",
    "\n",
    "# create the model\n",
    "model = PPO('CnnPolicy',\n",
    "            env=vec_env_1,\n",
    "            learning_rate=learning_rate,\n",
    "            batch_size=batch_size,\n",
    "            ent_coef=ent_coef,\n",
    "            n_epochs=n_epochs,\n",
    "            n_steps=n_steps,\n",
    "            tensorboard_log=tensorboard_log,\n",
    "            verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"./tmp/gym/\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "tb_log_name = 'task01-doorkey-6x6_cnn'\n",
    "\n",
    "# Create eval environment\n",
    "env = gym.make(env_id_1)\n",
    "env = ImgRGBImgPartialObsWrapper(env)\n",
    "eval_env = stable_baselines3.common.monitor.Monitor(env, log_dir)\n",
    "# Reset the environment\n",
    "eval_env.reset();\n",
    "\n",
    "# Stop training when the model reaches the reward threshold\n",
    "callback_on_best = StopTrainingOnRewardThreshold(reward_threshold=0.92, verbose=1)\n",
    "eval_callback = EvalCallback(eval_env, log_path=log_dir, callback_on_new_best=callback_on_best, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to ./tmp/log/task01-doorkey-6x6_cnn_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/inigo/Documents/cursos/Data Science/master_viu/work/minigrid/stable-baselines3/stable_baselines3/common/callbacks.py:345: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.vec_transpose.VecTransposeImage object at 0x13b418cd0> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x13b43f690>\n",
      "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 355        |\n",
      "|    ep_rew_mean          | 0.0164     |\n",
      "| time/                   |            |\n",
      "|    fps                  | 313        |\n",
      "|    iterations           | 10         |\n",
      "|    time_elapsed         | 130        |\n",
      "|    total_timesteps      | 40960      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00995178 |\n",
      "|    clip_fraction        | 0.0664     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.88      |\n",
      "|    explained_variance   | -0.0145    |\n",
      "|    learning_rate        | 5e-05      |\n",
      "|    loss                 | 0.042      |\n",
      "|    n_updates            | 36         |\n",
      "|    policy_gradient_loss | -0.00416   |\n",
      "|    value_loss           | 0.00138    |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 354         |\n",
      "|    ep_rew_mean          | 0.0212      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 304         |\n",
      "|    iterations           | 20          |\n",
      "|    time_elapsed         | 269         |\n",
      "|    total_timesteps      | 81920       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011094172 |\n",
      "|    clip_fraction        | 0.108       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.83       |\n",
      "|    explained_variance   | -4.36       |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0344     |\n",
      "|    n_updates            | 76          |\n",
      "|    policy_gradient_loss | -0.00769    |\n",
      "|    value_loss           | 0.000195    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 349         |\n",
      "|    ep_rew_mean          | 0.0353      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 301         |\n",
      "|    iterations           | 30          |\n",
      "|    time_elapsed         | 407         |\n",
      "|    total_timesteps      | 122880      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010752222 |\n",
      "|    clip_fraction        | 0.07        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.79       |\n",
      "|    explained_variance   | -0.156      |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | 0.00177     |\n",
      "|    n_updates            | 116         |\n",
      "|    policy_gradient_loss | -0.00585    |\n",
      "|    value_loss           | 0.000604    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=160000, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 360.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 360         |\n",
      "|    mean_reward          | 0           |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 160000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012168743 |\n",
      "|    clip_fraction        | 0.132       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.49       |\n",
      "|    explained_variance   | -2.67       |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | 0.00222     |\n",
      "|    n_updates            | 156         |\n",
      "|    policy_gradient_loss | -0.0096     |\n",
      "|    value_loss           | 0.000237    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 341      |\n",
      "|    ep_rew_mean     | 0.0595   |\n",
      "| time/              |          |\n",
      "|    fps             | 300      |\n",
      "|    iterations      | 40       |\n",
      "|    time_elapsed    | 544      |\n",
      "|    total_timesteps | 163840   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 321         |\n",
      "|    ep_rew_mean          | 0.122       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 302         |\n",
      "|    iterations           | 50          |\n",
      "|    time_elapsed         | 677         |\n",
      "|    total_timesteps      | 204800      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009417387 |\n",
      "|    clip_fraction        | 0.0912      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.56       |\n",
      "|    explained_variance   | 0.275       |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.00648    |\n",
      "|    n_updates            | 196         |\n",
      "|    policy_gradient_loss | -0.00692    |\n",
      "|    value_loss           | 0.00248     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 253         |\n",
      "|    ep_rew_mean          | 0.329       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 301         |\n",
      "|    iterations           | 60          |\n",
      "|    time_elapsed         | 813         |\n",
      "|    total_timesteps      | 245760      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009973964 |\n",
      "|    clip_fraction        | 0.104       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.41       |\n",
      "|    explained_variance   | 0.386       |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | 0.02        |\n",
      "|    n_updates            | 236         |\n",
      "|    policy_gradient_loss | -0.00592    |\n",
      "|    value_loss           | 0.0075      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 96.5        |\n",
      "|    ep_rew_mean          | 0.754       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 302         |\n",
      "|    iterations           | 70          |\n",
      "|    time_elapsed         | 947         |\n",
      "|    total_timesteps      | 286720      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012884958 |\n",
      "|    clip_fraction        | 0.134       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.07       |\n",
      "|    explained_variance   | 0.564       |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | 0.0573      |\n",
      "|    n_updates            | 276         |\n",
      "|    policy_gradient_loss | -0.00469    |\n",
      "|    value_loss           | 0.0104      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=320000, episode_reward=0.39 +/- 0.47\n",
      "Episode length: 221.20 +/- 170.01\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 221         |\n",
      "|    mean_reward          | 0.387       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 320000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016399076 |\n",
      "|    clip_fraction        | 0.134       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.794      |\n",
      "|    explained_variance   | 0.564       |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0155     |\n",
      "|    n_updates            | 312         |\n",
      "|    policy_gradient_loss | -0.00222    |\n",
      "|    value_loss           | 0.00577     |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 25.6        |\n",
      "|    ep_rew_mean          | 0.936       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 301         |\n",
      "|    iterations           | 80          |\n",
      "|    time_elapsed         | 1088        |\n",
      "|    total_timesteps      | 327680      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024684416 |\n",
      "|    clip_fraction        | 0.19        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.786      |\n",
      "|    explained_variance   | 0.539       |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | 0.0598      |\n",
      "|    n_updates            | 316         |\n",
      "|    policy_gradient_loss | -0.00592    |\n",
      "|    value_loss           | 0.0057      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 18.8        |\n",
      "|    ep_rew_mean          | 0.953       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 300         |\n",
      "|    iterations           | 90          |\n",
      "|    time_elapsed         | 1226        |\n",
      "|    total_timesteps      | 368640      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.031553127 |\n",
      "|    clip_fraction        | 0.207       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.457      |\n",
      "|    explained_variance   | 0.548       |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0652     |\n",
      "|    n_updates            | 356         |\n",
      "|    policy_gradient_loss | -0.00728    |\n",
      "|    value_loss           | 0.00257     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 18.1        |\n",
      "|    ep_rew_mean          | 0.955       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 300         |\n",
      "|    iterations           | 100         |\n",
      "|    time_elapsed         | 1364        |\n",
      "|    total_timesteps      | 409600      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.031645816 |\n",
      "|    clip_fraction        | 0.188       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.472      |\n",
      "|    explained_variance   | 0.641       |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | 0.0101      |\n",
      "|    n_updates            | 396         |\n",
      "|    policy_gradient_loss | -0.00721    |\n",
      "|    value_loss           | 0.00145     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15.5        |\n",
      "|    ep_rew_mean          | 0.961       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 299         |\n",
      "|    iterations           | 110         |\n",
      "|    time_elapsed         | 1503        |\n",
      "|    total_timesteps      | 450560      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024227137 |\n",
      "|    clip_fraction        | 0.173       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.35       |\n",
      "|    explained_variance   | 0.679       |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0252     |\n",
      "|    n_updates            | 436         |\n",
      "|    policy_gradient_loss | -0.00287    |\n",
      "|    value_loss           | 0.00121     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=480000, episode_reward=0.78 +/- 0.39\n",
      "Episode length: 79.40 +/- 140.30\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 79.4        |\n",
      "|    mean_reward          | 0.782       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 480000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.061060734 |\n",
      "|    clip_fraction        | 0.194       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.262      |\n",
      "|    explained_variance   | 0.514       |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0416     |\n",
      "|    n_updates            | 468         |\n",
      "|    policy_gradient_loss | 0.00545     |\n",
      "|    value_loss           | 0.00104     |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 14.8        |\n",
      "|    ep_rew_mean          | 0.963       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 299         |\n",
      "|    iterations           | 120         |\n",
      "|    time_elapsed         | 1640        |\n",
      "|    total_timesteps      | 491520      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030352779 |\n",
      "|    clip_fraction        | 0.218       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.305      |\n",
      "|    explained_variance   | 0.686       |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0344     |\n",
      "|    n_updates            | 476         |\n",
      "|    policy_gradient_loss | -0.0196     |\n",
      "|    value_loss           | 0.00107     |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 14.1       |\n",
      "|    ep_rew_mean          | 0.965      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 299        |\n",
      "|    iterations           | 130        |\n",
      "|    time_elapsed         | 1779       |\n",
      "|    total_timesteps      | 532480     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06209456 |\n",
      "|    clip_fraction        | 0.177      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.262     |\n",
      "|    explained_variance   | 0.656      |\n",
      "|    learning_rate        | 5e-05      |\n",
      "|    loss                 | -0.0775    |\n",
      "|    n_updates            | 516        |\n",
      "|    policy_gradient_loss | 0.0158     |\n",
      "|    value_loss           | 0.00272    |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 17.8        |\n",
      "|    ep_rew_mean          | 0.956       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 299         |\n",
      "|    iterations           | 140         |\n",
      "|    time_elapsed         | 1916        |\n",
      "|    total_timesteps      | 573440      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029347964 |\n",
      "|    clip_fraction        | 0.158       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.339      |\n",
      "|    explained_variance   | 0.633       |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0255     |\n",
      "|    n_updates            | 556         |\n",
      "|    policy_gradient_loss | -0.00263    |\n",
      "|    value_loss           | 0.000935    |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 14.9       |\n",
      "|    ep_rew_mean          | 0.963      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 298        |\n",
      "|    iterations           | 150        |\n",
      "|    time_elapsed         | 2056       |\n",
      "|    total_timesteps      | 614400     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03136676 |\n",
      "|    clip_fraction        | 0.199      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.224     |\n",
      "|    explained_variance   | 0.536      |\n",
      "|    learning_rate        | 5e-05      |\n",
      "|    loss                 | -0.087     |\n",
      "|    n_updates            | 596        |\n",
      "|    policy_gradient_loss | 0.000908   |\n",
      "|    value_loss           | 0.00071    |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=640000, episode_reward=0.77 +/- 0.39\n",
      "Episode length: 82.40 +/- 138.82\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 82.4        |\n",
      "|    mean_reward          | 0.774       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 640000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017059583 |\n",
      "|    clip_fraction        | 0.17        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.443      |\n",
      "|    explained_variance   | 0.632       |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | 0.0501      |\n",
      "|    n_updates            | 624         |\n",
      "|    policy_gradient_loss | -0.00689    |\n",
      "|    value_loss           | 0.0037      |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 21.2       |\n",
      "|    ep_rew_mean          | 0.947      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 298        |\n",
      "|    iterations           | 160        |\n",
      "|    time_elapsed         | 2196       |\n",
      "|    total_timesteps      | 655360     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04584595 |\n",
      "|    clip_fraction        | 0.243      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.364     |\n",
      "|    explained_variance   | 0.574      |\n",
      "|    learning_rate        | 5e-05      |\n",
      "|    loss                 | -0.0365    |\n",
      "|    n_updates            | 636        |\n",
      "|    policy_gradient_loss | -0.0234    |\n",
      "|    value_loss           | 0.00145    |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 14.8        |\n",
      "|    ep_rew_mean          | 0.963       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 297         |\n",
      "|    iterations           | 170         |\n",
      "|    time_elapsed         | 2336        |\n",
      "|    total_timesteps      | 696320      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.050388347 |\n",
      "|    clip_fraction        | 0.242       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.307      |\n",
      "|    explained_variance   | 0.642       |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0431     |\n",
      "|    n_updates            | 676         |\n",
      "|    policy_gradient_loss | -0.00217    |\n",
      "|    value_loss           | 0.000961    |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 15.9       |\n",
      "|    ep_rew_mean          | 0.96       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 298        |\n",
      "|    iterations           | 180        |\n",
      "|    time_elapsed         | 2473       |\n",
      "|    total_timesteps      | 737280     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03651467 |\n",
      "|    clip_fraction        | 0.201      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.372     |\n",
      "|    explained_variance   | 0.633      |\n",
      "|    learning_rate        | 5e-05      |\n",
      "|    loss                 | -0.000683  |\n",
      "|    n_updates            | 716        |\n",
      "|    policy_gradient_loss | -0.0136    |\n",
      "|    value_loss           | 0.00141    |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 13.7        |\n",
      "|    ep_rew_mean          | 0.966       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 298         |\n",
      "|    iterations           | 190         |\n",
      "|    time_elapsed         | 2610        |\n",
      "|    total_timesteps      | 778240      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027231622 |\n",
      "|    clip_fraction        | 0.146       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.15       |\n",
      "|    explained_variance   | 0.806       |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.00665    |\n",
      "|    n_updates            | 756         |\n",
      "|    policy_gradient_loss | -0.0175     |\n",
      "|    value_loss           | 0.000355    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=800000, episode_reward=0.77 +/- 0.38\n",
      "Episode length: 84.20 +/- 137.95\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 84.2        |\n",
      "|    mean_reward          | 0.769       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 800000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.034039445 |\n",
      "|    clip_fraction        | 0.204       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.307      |\n",
      "|    explained_variance   | 0.661       |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0108     |\n",
      "|    n_updates            | 780         |\n",
      "|    policy_gradient_loss | -0.0171     |\n",
      "|    value_loss           | 0.00105     |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 26.8       |\n",
      "|    ep_rew_mean          | 0.933      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 298        |\n",
      "|    iterations           | 200        |\n",
      "|    time_elapsed         | 2742       |\n",
      "|    total_timesteps      | 819200     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08142426 |\n",
      "|    clip_fraction        | 0.107      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.143     |\n",
      "|    explained_variance   | 0.622      |\n",
      "|    learning_rate        | 5e-05      |\n",
      "|    loss                 | -0.069     |\n",
      "|    n_updates            | 796        |\n",
      "|    policy_gradient_loss | -0.0112    |\n",
      "|    value_loss           | 0.000575   |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 17.4       |\n",
      "|    ep_rew_mean          | 0.957      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 298        |\n",
      "|    iterations           | 210        |\n",
      "|    time_elapsed         | 2877       |\n",
      "|    total_timesteps      | 860160     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05847463 |\n",
      "|    clip_fraction        | 0.286      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.278     |\n",
      "|    explained_variance   | 0.356      |\n",
      "|    learning_rate        | 5e-05      |\n",
      "|    loss                 | -0.0704    |\n",
      "|    n_updates            | 836        |\n",
      "|    policy_gradient_loss | -0.0165    |\n",
      "|    value_loss           | 0.00117    |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 16.9        |\n",
      "|    ep_rew_mean          | 0.958       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 299         |\n",
      "|    iterations           | 220         |\n",
      "|    time_elapsed         | 3011        |\n",
      "|    total_timesteps      | 901120      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.080422804 |\n",
      "|    clip_fraction        | 0.192       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.265      |\n",
      "|    explained_variance   | 0.542       |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0127     |\n",
      "|    n_updates            | 876         |\n",
      "|    policy_gradient_loss | 0.00602     |\n",
      "|    value_loss           | 0.000779    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 13.9        |\n",
      "|    ep_rew_mean          | 0.965       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 299         |\n",
      "|    iterations           | 230         |\n",
      "|    time_elapsed         | 3150        |\n",
      "|    total_timesteps      | 942080      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029643726 |\n",
      "|    clip_fraction        | 0.108       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.142      |\n",
      "|    explained_variance   | 0.675       |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0615     |\n",
      "|    n_updates            | 916         |\n",
      "|    policy_gradient_loss | -0.0184     |\n",
      "|    value_loss           | 0.000475    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=960000, episode_reward=0.77 +/- 0.39\n",
      "Episode length: 82.00 +/- 139.02\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 82          |\n",
      "|    mean_reward          | 0.775       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 960000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025948621 |\n",
      "|    clip_fraction        | 0.128       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.139      |\n",
      "|    explained_variance   | 0.744       |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | 0.0832      |\n",
      "|    n_updates            | 936         |\n",
      "|    policy_gradient_loss | -0.00975    |\n",
      "|    value_loss           | 0.000342    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 14.1        |\n",
      "|    ep_rew_mean          | 0.965       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 298         |\n",
      "|    iterations           | 240         |\n",
      "|    time_elapsed         | 3296        |\n",
      "|    total_timesteps      | 983040      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027902976 |\n",
      "|    clip_fraction        | 0.181       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.228      |\n",
      "|    explained_variance   | 0.703       |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | 0.113       |\n",
      "|    n_updates            | 956         |\n",
      "|    policy_gradient_loss | -0.0162     |\n",
      "|    value_loss           | 0.000683    |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x13b418150>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_timesteps = 1000000\n",
    "log_interval = 10\n",
    "\n",
    "model.learn(total_timesteps=total_timesteps,\n",
    "            log_interval=log_interval,\n",
    "            tb_log_name = tb_log_name,\n",
    "            callback=eval_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the agent\n",
    "tb_log_name = 'task01-doorkey-6x6_cnn_local'\n",
    "model.save(tb_log_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean reward: 0.9217500060796737 +/- 0.21\n"
     ]
    }
   ],
   "source": [
    "# We create a separate environment for evaluation\n",
    "eval_env = ImgRGBImgPartialObsWrapper(gym.make(env_id_1))\n",
    "eval_env.seed(10000+randint(0, 10))\n",
    "eval_env.reset()\n",
    "# Random Agent, before training\n",
    "mean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=20)\n",
    "print(f'Mean reward: {mean_reward} +/- {std_reward:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reward: 0.97\n",
      "Total length: 12\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<video alt=\"test\" autoplay \n",
       "                loop controls style=\"height: 400px;\">\n",
       "                <source src=\"data:video/mp4;base64,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\" type=\"video/mp4\" />\n",
       "             </video>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_env = gen_wrapped_env_cnn(env_id_1)\n",
    "test_env.seed(10000+randint(0, 10))\n",
    "observation = test_env.reset()\n",
    "\n",
    "done = False\n",
    "episode_reward = 0\n",
    "episode_length = 0\n",
    "\n",
    "while not done:\n",
    "  #test_env.render()\n",
    "  action, states = model.predict(observation, deterministic=False)\n",
    "  observation, reward, done, info = test_env.step(action)\n",
    "  episode_reward += reward\n",
    "  episode_length += 1\n",
    "\n",
    "print('Total reward:', episode_reward)\n",
    "print('Total length:', episode_length)\n",
    "\n",
    "test_env.close()\n",
    "show_video()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "test_minigrid_sb3_curriculum.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "51a1b245ec7fc72d2061587b663ba1f584a95fe24cf1a61a943e144e8f966db4"
  },
  "kernelspec": {
   "display_name": "minigrid",
   "language": "python",
   "name": "minigrid"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
