import numpy as np
import matplotlib.pyplot as plt
from pprint import pprint

get_ipython().run_line_magic("matplotlib", " inline")
plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots
plt.rcParams['image.interpolation'] = 'nearest'
plt.rcParams['image.cmap'] = 'gray'

# for auto-reloading external modules
# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython
get_ipython().run_line_magic("load_ext", " autoreload")
get_ipython().run_line_magic("autoreload", " 2")


# @title Install dependencies
get_ipython().getoutput("pip install rarfile --quiet")
get_ipython().getoutput("pip install stable-baselines3 > /dev/null")
get_ipython().getoutput("pip install box2d-py > /dev/null")
get_ipython().getoutput("pip install gym gym-minigrid pyvirtualdisplay > /dev/null 2>&1")
get_ipython().getoutput("sudo apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1")


import gym
import gym_minigrid

env = gym.make('MiniGrid-Empty-5x5-v0')
env.reset()
before_img = env.render('rgb_array')
action = env.actions.forward
obs, reward, done, info = env.step(action)
after_img = env.render('rgb_array')

plt.imshow(np.concatenate([before_img, after_img], 1));


from gym_minigrid.wrappers import FlatObsWrapper
# Convert MiniGrid Environment with Flat Observable 
env = FlatObsWrapper(gym.make('MiniGrid-Empty-8x8-v0'))

# Reset the environment
env.reset()

# Select the action right
action = env.actions.right

# Take a step in the environment and store it in appropriate variables
obs, reward, done, info = env.step(action)

# Render the current state of the environment
img = env.render('rgb_array')
################# YOUR CODE ENDS HERE ###############################

print('Observation:', obs, ', Observation Shape: ', obs.shape)
print('Reward:', reward)
print('Done:', done)
print('Info:', info)
print('Image shape:', img.shape)
plt.imshow(img);


# Imports
import time
import io
import os
import glob
import torch
import base64
import stable_baselines3

import numpy as np
import matplotlib.pyplot as plt

from stable_baselines3 import PPO, DQN, A2C
from stable_baselines3.common.results_plotter import ts2xy, load_results
from stable_baselines3.common.callbacks import EvalCallback
from stable_baselines3.common.env_util import make_atari_env
from stable_baselines3.common.evaluation import evaluate_policy
from stable_baselines3.common.utils import set_random_seed
from stable_baselines3.common.vec_env import SubprocVecEnv, DummyVecEnv
from stable_baselines3.common.env_util import make_vec_env

import gym
from gym import spaces
from gym.wrappers import Monitor
import gym_minigrid
from gym_minigrid.wrappers import FlatObsWrapper, ImgObsWrapper


import base64
import glob
import io
from IPython.display import HTML
from IPython import display 

def show_video():
    mp4list = glob.glob('video/*.mp4')
    if len(mp4list) > 0:
        mp4 = mp4list[0]
        video = io.open(mp4, 'r+b').read()
        encoded = base64.b64encode(video)
        display.display(HTML(data='''<video alt="test" autoplay 
                loop controls style="height: 400px;">
                <source src="data:video/mp4;base64,{0}" type="video/mp4" />
             </video>'''.format(encoded.decode('ascii'))))
    else:
        print("Could not find video")



from gym.wrappers import Monitor

# Monitor is a gym wrapper, which helps easy rendering of videos of the wrapped environment.
def wrap_env(env):
    env = Monitor(env, './video', force=True)
    return env

def gen_wrapped_env(env_name):
    return wrap_env(FlatObsWrapper(gym.make(env_name)))


# By default, we use a DummyVecEnv as it is usually faster (cf doc)
num_cpu = 16  # Number of processes to use
env_id = 'MiniGrid-Empty-6x6-v0'

vec_env = make_vec_env(env_id, n_envs=num_cpu, wrapper_class=FlatObsWrapper)

# multivect model
model = A2C('MlpPolicy', env=vec_env, verbose=0)


# We create a separate environment for evaluation
eval_env = FlatObsWrapper(gym.make(env_id))

# Random Agent, before training
mean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=10)
print(f'Mean reward: {mean_reward} +/- {std_reward:.2f}')


n_timesteps = 25000


# Single Process RL Training
env = FlatObsWrapper(gym.make(env_id))
single_process_model = A2C('MlpPolicy', env=env, verbose=0)

start_time = time.time()
single_process_model.learn(n_timesteps)
total_time_single = time.time() - start_time

print(f"Took {total_time_single:.2f}s for single process version - {n_timesteps / total_time_single:.2f} FPS")


# We create a separate environment for evaluation
eval_env = FlatObsWrapper(gym.make(env_id))

# Agent after training
mean_reward, std_reward = evaluate_policy(single_process_model, eval_env, n_eval_episodes=50)
print(f'Mean reward: {mean_reward} +/- {std_reward:.2f}')


# Multiprocessed RL Training
start_time = time.time()
model.learn(n_timesteps)
total_time_multi = time.time() - start_time

print(f"Took {total_time_multi:.2f}s for multiprocessed version - {n_timesteps / total_time_multi:.2f} FPS")


# We create a separate environment for evaluation
eval_env = FlatObsWrapper(gym.make(env_id))

# Random Agent, before training
mean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=50)
print(f'Mean reward: {mean_reward} +/- {std_reward:.2f}')


# By default, we use a DummyVecEnv as it is usually faster (cf doc)
num_cpu = 16  # Number of processes to use
env_id = 'MiniGrid-Empty-16x16-v0'
vec_env = make_vec_env(env_id, n_envs=num_cpu, wrapper_class=FlatObsWrapper)


#nn_layers = [64,64] #This is the configuration of your neural network. Currently, we have two layers, each consisting of 64 neurons.
                    #If you want three layers with 64 neurons each, set the value to [64,64,64] and so on.

#learning_rate = 0.001 #This is the step-size with which the gradient descent is carried out.
                      #Tip: Use smaller step-sizes for larger networks.
learning_rate = 0.00005
n_steps = 256
batch_size = 16
ent_coef = 0.001
n_epochs = 4
#policy_kwargs = dict(activation_fn=torch.nn.ReLU,net_arch=nn_layers)


tensorboard_log = "./tmp/log/"
os.makedirs(tensorboard_log, exist_ok=True)
# Reset the environment
vec_env.reset()

# create the model
model = PPO('MlpPolicy', env=vec_env, learning_rate=learning_rate, batch_size=batch_size, ent_coef=ent_coef, n_epochs=n_epochs, n_steps=n_steps, tensorboard_log=tensorboard_log, verbose=1)


log_dir = "./tmp/gym/"
os.makedirs(log_dir, exist_ok=True)

# Create eval environment
env = gym.make(env_id)
env = FlatObsWrapper(env)
env = stable_baselines3.common.monitor.Monitor(env, log_dir)
# Reset the environment
env.reset();
#For evaluating the performance of the agent periodically and logging the results.
callback = EvalCallback(env, log_path = log_dir, deterministic=True)


total_timesteps = 100000
log_interval = 1
tb_log_name = env_id

model.learn(total_timesteps=total_timesteps,
            log_interval=log_interval,
            tb_log_name = tb_log_name,
            callback=callback)
# The performance of the training will be printed every 10 episodes. Change it to 1, if you wish to
# view the performance at every training episode.


# Save the agent
model.save(env_id)


model = PPO.load(path=env_id)


# We create a separate environment for evaluation
eval_env = FlatObsWrapper(gym.make(env_id))

# Random Agent, before training
mean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=50)
print(f'Mean reward: {mean_reward} +/- {std_reward:.2f}')


env_id = 'MiniGrid-Empty-Random-6x6-v0'
# We create a separate environment for evaluation
eval_env = FlatObsWrapper(gym.make(env_id))

# Random Agent, before training
mean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=50)
print(f'Mean reward: {mean_reward} +/- {std_reward:.2f}')


env_id = 'MiniGrid-Empty-16x16-v0'
eval_env = gym.make(env_id)
eval_env.reset()
before_img = eval_env.render('rgb_array')

plt.imshow(before_img);


env_id = 'MiniGrid-Empty-16x16-v0'
#env = PyTorchObsWrapper(gym.make(env_id)) # Observation Shape:  (3, 80, 60)
#env = wrap_env(gym.make(env_id)) # Observation Shape:  (60, 80, 3)
#env = gym.make(env_id) # Observation Shape:  (60, 80, 3)
env = gen_wrapped_env(env_id) # Observation Shape:  (3, 80, 60)
            
#env = gym.make(env_id)
obs = env.reset()
print('Observation:', obs, ', Observation Shape: ', obs.shape)

# Select the action right
#action = env.actions.turn_right

# Take a step in the environment and store it in appropriate variables
#obs, reward, done, info = env.step(action)

#obs = obs.transpose(0,2,1)
#print('Observation:', obs, ', Observation Shape: ', obs.shape)


env_id = 'MiniGrid-Empty-16x16-v0'
#env_id = 'MiniGrid-Empty-Random-6x6-v0'
#env_id = 'MiniGrid-Empty-8x8-v0'

test_env = gen_wrapped_env(env_id)
observation = test_env.reset()

done = False
episode_reward = 0
episode_length = 0

while not done:
  #test_env.render()
  action, states = model.predict(observation, deterministic=True)
  observation, reward, done, info = test_env.step(action)
  episode_reward += reward
  episode_length += 1

print('Total reward:', episode_reward)
print('Total length:', episode_length)

test_env.close()
show_video()


env = gym.make('MiniGrid-DistShift1-v0')
env.reset()
before_img = env.render('rgb_array')
action = env.actions.forward
obs, reward, done, info = env.step(action)
after_img = env.render('rgb_array')

plt.imshow(np.concatenate([before_img, after_img], 1));


# By default, we use a DummyVecEnv as it is usually faster (cf doc)
num_cpu = 16  # Number of processes to use
env_id = 'MiniGrid-DistShift1-v0'
vec_env = make_vec_env(env_id, n_envs=num_cpu, wrapper_class=FlatObsWrapper)

learning_rate = 0.00005
n_steps = 256
batch_size = 16
ent_coef = 0.001
n_epochs = 4

tensorboard_log = "./tmp/log/"
os.makedirs(tensorboard_log, exist_ok=True)
# Reset the environment
vec_env.reset()

# create the model
model = PPO('MlpPolicy',
            env=vec_env,
            learning_rate=learning_rate,
            batch_size=batch_size,
            ent_coef=ent_coef,
            n_epochs=n_epochs,
            n_steps=n_steps,
            tensorboard_log=tensorboard_log,
            verbose=1)


log_dir = "./tmp/gym/"
os.makedirs(log_dir, exist_ok=True)

# Create eval environment
env = gym.make(env_id)
env = FlatObsWrapper(env)
env = stable_baselines3.common.monitor.Monitor(env, log_dir)
# Reset the environment
env.reset();
#For evaluating the performance of the agent periodically and logging the results.
callback = EvalCallback(env, log_path = log_dir, deterministic=True)


total_timesteps = 500000
log_interval = 10
tb_log_name = env_id

model.learn(total_timesteps=total_timesteps,
            log_interval=log_interval,
            tb_log_name = tb_log_name,
            callback=callback)
# The performance of the training will be printed every 10 episodes. Change it to 1, if you wish to
# view the performance at every training episode.


# Save the agent
model.save(env_id) 


# We create a separate environment for evaluation
eval_env = FlatObsWrapper(gym.make(env_id))

# Random Agent, before training
mean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=50)
print(f'Mean reward: {mean_reward} +/- {std_reward:.2f}')


# number of timesteps to add
total_timesteps = 200000
log_interval = 10

# load the model on previous vectorized env and tensorboard log directory
model = PPO.load(path=env_id, env=vec_env, tensorboard_log=tensorboard_log)
#model.set_env(vec_env)

# resume training using same tensorboard run without resetting the timesteps
model.learn(total_timesteps=total_timesteps,
            log_interval=log_interval,
            tb_log_name=tb_log_name,
            reset_num_timesteps=False)


# We create a separate environment for evaluation
eval_env = FlatObsWrapper(gym.make(env_id))

# Random Agent, before training
mean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=50)
print(f'Mean reward: {mean_reward} +/- {std_reward:.2f}')


# Save the agent
model.save(env_id+'_2') 


# reload the model in case we don't have it
model = PPO.load('MiniGrid-DistShift1-v0_2')

env_id = 'MiniGrid-DistShift1-v0'

test_env = gen_wrapped_env(env_id)
observation = test_env.reset()

done = False
episode_reward = 0
episode_length = 0

while not done:
  #test_env.render()
  action, states = model.predict(observation, deterministic=True)
  observation, reward, done, info = test_env.step(action)
  episode_reward += reward
  episode_length += 1

print('Total reward:', episode_reward)
print('Total length:', episode_length)

test_env.close()
show_video()


# reload the model in case we don't have it
#model = PPO.load('MiniGrid-DistShift1-v0_2')

env_id = 'MiniGrid-DistShift2-v0'

test_env = gen_wrapped_env(env_id)
observation = test_env.reset()

done = False
episode_reward = 0
episode_length = 0

while not done:
  #test_env.render()
  action, states = model.predict(observation, deterministic=True)
  observation, reward, done, info = test_env.step(action)
  episode_reward += reward
  episode_length += 1

print('Total reward:', episode_reward)
print('Total length:', episode_length)

test_env.close()
show_video()


# By default, we use a DummyVecEnv as it is usually faster (cf doc)
num_cpu = 16  # Number of processes to use
new_env_id = 'MiniGrid-DistShift2-v0'
vec_env = make_vec_env(new_env_id, n_envs=num_cpu, wrapper_class=FlatObsWrapper)


# previous saved environment
prev_env_id = env_id + '_2'

# load the model on previous vectorized env and tensorboard log directory
model = PPO.load(path=prev_env_id, env=vec_env, tensorboard_log=tensorboard_log)
#model.set_env(vec_env)


# number of timesteps to add
total_timesteps = 500000
log_interval = 10

# resume training using same tensorboard run without resetting the timesteps
model.learn(total_timesteps=total_timesteps,
            log_interval=log_interval,
            tb_log_name=tb_log_name,
            reset_num_timesteps=False, 
            callback=callback)


# Save the agent
new_env_id2 = 'MiniGrid-DistShift2_with_1-v0'
model.save(new_env_id2) 


# uncomment to load the last saved model in case the notebook kernel is restarted
#model = PPO.load(path=new_env_id2, env=vec_env, tensorboard_log=tensorboard_log)

# number of timesteps to add
total_timesteps = 500000
log_interval = 10

# resume training using same tensorboard run without resetting the timesteps
model.learn(total_timesteps=total_timesteps,
            log_interval=log_interval,
            tb_log_name=tb_log_name,
            reset_num_timesteps=False, 
            callback=callback)


# Save the agent
new_env_id3 = 'MiniGrid-DistShift2_with_1-v0_2'
model.save(new_env_id3) 


# uncomment to load the last saved model in case the notebook kernel is restarted, etc.
model = PPO.load(path=new_env_id3)

# We create a separate environment for evaluation
eval_env = FlatObsWrapper(gym.make('MiniGrid-DistShift2-v0'))

# Random Agent, before training
mean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=50)
print(f'Mean reward: {mean_reward} +/- {std_reward:.2f}')


# uncomment to load the last saved model in case the notebook kernel is restarted, etc.
# model = PPO.load(path=new_env_id3)

# We create a separate environment for evaluation
eval_env = FlatObsWrapper(gym.make('MiniGrid-DistShift1-v0'))

# Random Agent, before training
mean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=50)
print(f'Mean reward: {mean_reward} +/- {std_reward:.2f}')


env_id = 'MiniGrid-DistShift2-v0'

test_env = gen_wrapped_env(env_id)
observation = test_env.reset()

done = False
episode_reward = 0
episode_length = 0

while not done:
  #test_env.render()
  action, states = model.predict(observation, deterministic=True)
  observation, reward, done, info = test_env.step(action)
  episode_reward += reward
  episode_length += 1

print('Total reward:', episode_reward)
print('Total length:', episode_length)

test_env.close()
show_video()


env_id = 'MiniGrid-DistShift1-v0'

test_env = gen_wrapped_env(env_id)
observation = test_env.reset()

done = False
episode_reward = 0
episode_length = 0

while not done:
  #test_env.render()
  action, states = model.predict(observation, deterministic=True)
  observation, reward, done, info = test_env.step(action)
  episode_reward += reward
  episode_length += 1

print('Total reward:', episode_reward)
print('Total length:', episode_length)

test_env.close()
show_video()


nn_layers = [64,64] #This is the configuration of your neural network. Currently, we have two layers, each consisting of 64 neurons.
                    #If you want three layers with 64 neurons each, set the value to [64,64,64] and so on.

learning_rate = 0.001 #This is the step-size with which the gradient descent is carried out.
                      #Tip: Use smaller step-sizes for larger networks.


log_dir = "/tmp/gym/"
os.makedirs(log_dir, exist_ok=True)

# Create environment
#env = gym.make('LunarLander-v2')
# Make a new environment MiniGrid-Empty-8x8-v0
#env = gym.make('MiniGrid-DoorKey-5x5-v0')
#env = gym.make('MiniGrid-Empty-8x8-v0')
env = gym.make('MiniGrid-FourRooms-v0')
env = FlatObsWrapper(env)

#env = FlatObsWrapper(gym_minigrid.wrappers.ImgObsWrapper(gym_minigrid.wrappers.RGBImgObsWrapper(env)))
# Reset the environment
env.reset();

env = stable_baselines3.common.monitor.Monitor(env, log_dir )

callback = EvalCallback(env,log_path = log_dir, deterministic=True) #For evaluating the performance of the agent periodically and logging the results.
#policy_kwargs = dict(activation_fn=torch.nn.ReLU,net_arch=nn_layers)
 
model = PPO("MlpPolicy", env, learning_rate=learning_rate) 

# You can also experiment with other RL algorithms like A2C, PPO, DDPG etc. Refer to  https://stable-baselines3.readthedocs.io/en/master/guide/examples.html
#for documentation. For example, if you would like to run DDPG, just replace "DQN" above with "DDPG".


#test_env = wrap_env(gym.make('MiniGrid-Empty-8x8-v0'))
#test_env = gen_wrapped_env('MiniGrid-DoorKey-5x5-v0')
#test_env = gen_wrapped_env('MiniGrid-Empty-8x8-v0')
test_env = gen_wrapped_env('MiniGrid-FourRooms-v0')

observation = test_env.reset()

done = False
episode_reward = 0
episode_length = 0

while not done:
  #test_env.render()
  action, states = model.predict(observation, deterministic=True)
  observation, reward, done, info = test_env.step(action)
  episode_reward += reward
  episode_length += 1

print('Total reward:', episode_reward)
print('Total length:', episode_length)

test_env.close()
show_video()


total_timesteps = 1000000
log_interval = 10

model.learn(total_timesteps=total_timesteps, log_interval=log_interval, callback=callback)
# The performance of the training will be printed every 10 episodes. Change it to 1, if you wish to
# view the performance at every training episode.


#env = gen_wrapped_env('MiniGrid-Empty-8x8-v0')
#env = gen_wrapped_env('MiniGrid-DoorKey-5x5-v0')
env = gen_wrapped_env('MiniGrid-FourRooms-v0')
observation = env.reset()

while True:
  #env.render()
  action, _states = model.predict(observation, deterministic=True)
  observation, reward, done, info = env.step(action)
  if done:
    break;

env.close()
show_video()
