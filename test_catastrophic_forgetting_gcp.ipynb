{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MNF5-qJ7B0Q3"
   },
   "source": [
    "# Testing Stable Baselines3 with gym-MiniGrid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AdM2FnEJB0Q8"
   },
   "source": [
    "## Basic Jupyter Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 319,
     "status": "ok",
     "timestamp": 1647123362972,
     "user": {
      "displayName": "Iñigo",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "14378798962183195551"
     },
     "user_tz": -60
    },
    "id": "aycUmr6OB0Q8"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pprint import pprint\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ef0DdE0b4pLd"
   },
   "source": [
    "## Import Stable-Baselines3 and initial set-up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PLM4YYcL5rBt"
   },
   "source": [
    "Import libraries and classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "YgenDMtf4pLe"
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import time\n",
    "import io\n",
    "import os\n",
    "import glob\n",
    "import torch\n",
    "import base64\n",
    "import stable_baselines3\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from random import randint \n",
    "from stable_baselines3 import PPO, DQN, A2C\n",
    "from stable_baselines3.common.results_plotter import ts2xy, load_results\n",
    "from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnRewardThreshold\n",
    "from stable_baselines3.common.env_util import make_atari_env\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.utils import set_random_seed\n",
    "from stable_baselines3.common.vec_env import SubprocVecEnv, DummyVecEnv\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "import gym\n",
    "from gym import spaces\n",
    "from gym.wrappers import Monitor\n",
    "import gym_minigrid\n",
    "from gym_minigrid.wrappers import FlatObsWrapper, ImgObsWrapper, RGBImgPartialObsWrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define wrapper for CNN Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ImgRGBImgPartialObsWrapper(env):\n",
    "    return ImgObsWrapper(RGBImgPartialObsWrapper(env))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "28U_WEp25rBu"
   },
   "source": [
    "Define the video function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "d7eCH8Kf4pLf"
   },
   "outputs": [],
   "source": [
    "import base64\n",
    "import glob\n",
    "import io\n",
    "from IPython.display import HTML\n",
    "from IPython import display \n",
    "\n",
    "def show_video():\n",
    "    mp4list = glob.glob('video/*.mp4')\n",
    "    if len(mp4list) > 0:\n",
    "        mp4 = mp4list[0]\n",
    "        video = io.open(mp4, 'r+b').read()\n",
    "        encoded = base64.b64encode(video)\n",
    "        display.display(HTML(data='''<video alt=\"test\" autoplay \n",
    "                loop controls style=\"height: 400px;\">\n",
    "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
    "             </video>'''.format(encoded.decode('ascii'))))\n",
    "    else:\n",
    "        print(\"Could not find video\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KchGuXpd5rBv"
   },
   "source": [
    "Define the rendering wrappers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Gdhk3Oep4pLf"
   },
   "outputs": [],
   "source": [
    "from gym.wrappers import Monitor\n",
    "\n",
    "# Monitor is a gym wrapper, which helps easy rendering of videos of the wrapped environment.\n",
    "def wrap_env(env):\n",
    "    env = Monitor(env, './video', force=True)\n",
    "    return env\n",
    "\n",
    "def gen_wrapped_env(env_name):\n",
    "    return wrap_env(FlatObsWrapper(gym.make(env_name)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "executionInfo": {
     "elapsed": 900,
     "status": "ok",
     "timestamp": 1647083269049,
     "user": {
      "displayName": "Iñigo",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "14378798962183195551"
     },
     "user_tz": -60
    },
    "id": "B21JwGYn5rBv",
    "outputId": "54dfa526-2621-4f91-df2b-e4ff7a447aad"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from random import randint\n",
    "#env_id = 'MiniGrid-Empty-16x16-v0'\n",
    "env_id = 'MiniGrid-DoorKey-6x6-v0'\n",
    "#env_id = 'MiniGrid-DistShift1-v0'\n",
    "#env_id ='MiniGrid-UnlockPickup-v0'\n",
    "#env_id = 'MiniGrid-RedBlueDoors-6x6-v0'\n",
    "#env_id = 'MiniGrid-LavaGapS6-v0'\n",
    "\n",
    "eval_env = gym.make(env_id)\n",
    "eval_env.seed(10000+randint(0, 10))\n",
    "eval_env.reset()\n",
    "before_img = eval_env.render('rgb_array')\n",
    "\n",
    "plt.imshow(before_img);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "umY09KJP5rCI"
   },
   "source": [
    "# Catastrophic Forgetting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yPq1XkeL5rCI"
   },
   "source": [
    "## Define the environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eYzbUoTS5rCI"
   },
   "outputs": [],
   "source": [
    "# By default, we use a DummyVecEnv as it is usually faster (cf doc)\n",
    "num_cpu = 16  # Number of processes to use\n",
    "\n",
    "env_id_1 = 'MiniGrid-DoorKey-6x6-v0'\n",
    "vec_env_1 = make_vec_env(env_id_1, n_envs=num_cpu, wrapper_class=FlatObsWrapper, seed=10000)\n",
    "\n",
    "env_id_2 = 'MiniGrid-LavaGapS6-v0'\n",
    "vec_env_2 = make_vec_env(env_id_2, n_envs=num_cpu, wrapper_class=FlatObsWrapper, seed=5000)\n",
    "\n",
    "env_id_3 = 'MiniGrid-RedBlueDoors-6x6-v0'\n",
    "vec_env_3 = make_vec_env(env_id_3, n_envs=num_cpu, wrapper_class=FlatObsWrapper, seed=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 197,
     "status": "ok",
     "timestamp": 1647085386489,
     "user": {
      "displayName": "Iñigo",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "14378798962183195551"
     },
     "user_tz": -60
    },
    "id": "vT24ysNm79CE",
    "outputId": "8f710f58-57f4-44c7-aebf-d24c88c99da3"
   },
   "outputs": [],
   "source": [
    "from stable_baselines3.common.utils import get_device\n",
    "get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "94zAf0-I5rCI"
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.00005\n",
    "n_steps = 256\n",
    "batch_size = 16\n",
    "ent_coef = 0.001\n",
    "n_epochs = 4\n",
    "\n",
    "tensorboard_log = \"./tmp/log/\"\n",
    "os.makedirs(tensorboard_log, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SDhlDj-J5rCI"
   },
   "source": [
    "## Learn first environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9700,
     "status": "ok",
     "timestamp": 1646782989154,
     "user": {
      "displayName": "Iñigo",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "14378798962183195551"
     },
     "user_tz": -60
    },
    "id": "fgNWgfba5rCI",
    "outputId": "553d07ab-f3db-4c45-e45a-b2624d78a477",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Reset the environment\n",
    "vec_env_1.reset()\n",
    "\n",
    "# create the model\n",
    "model = PPO('MlpPolicy',\n",
    "            env=vec_env_1,\n",
    "            learning_rate=learning_rate,\n",
    "            batch_size=batch_size,\n",
    "            ent_coef=ent_coef,\n",
    "            n_epochs=n_epochs,\n",
    "            n_steps=n_steps,\n",
    "            tensorboard_log=tensorboard_log,\n",
    "            verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a75Hyh4n5rCJ"
   },
   "outputs": [],
   "source": [
    "log_dir = \"./tmp/gym/\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "tb_log_name = 'task01-doorkey-6x6'\n",
    "\n",
    "# Create eval environment\n",
    "env = gym.make(env_id_1)\n",
    "env = FlatObsWrapper(env)\n",
    "eval_env = stable_baselines3.common.monitor.Monitor(env, log_dir)\n",
    "# Reset the environment\n",
    "eval_env.reset();\n",
    "\n",
    "# Stop training when the model reaches the reward threshold\n",
    "callback_on_best = StopTrainingOnRewardThreshold(reward_threshold=0.92, verbose=1)\n",
    "eval_callback = EvalCallback(eval_env, log_path=log_dir, callback_on_new_best=callback_on_best, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cBedyr7x5rCJ",
    "outputId": "77956ad4-b758-4779-c8d2-3267555ba0b6",
    "tags": []
   },
   "outputs": [],
   "source": [
    "total_timesteps = 1000000\n",
    "log_interval = 10\n",
    "\n",
    "model.learn(total_timesteps=total_timesteps,\n",
    "            log_interval=log_interval,\n",
    "            tb_log_name = tb_log_name,\n",
    "            callback=eval_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DO-9hK8frbSj",
    "outputId": "772b6bd5-3a8e-40a9-bb47-a1bf792b7fdb"
   },
   "outputs": [],
   "source": [
    "print(tb_log_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4qWDyB355rCJ"
   },
   "outputs": [],
   "source": [
    "# Save the agent\n",
    "model.save(tb_log_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XtIhYAxd5rCJ"
   },
   "outputs": [],
   "source": [
    "#model = PPO.load(path=tb_log_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iz6ij9Nt5rCJ",
    "outputId": "85503a8d-7a79-493b-d06f-bd9ccbb0dc83"
   },
   "outputs": [],
   "source": [
    "# We create a separate environment for evaluation\n",
    "eval_env = FlatObsWrapper(gym.make(env_id_1))\n",
    "\n",
    "# Random Agent, before training\n",
    "mean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=20)\n",
    "print(f'Mean reward: {mean_reward} +/- {std_reward:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3qj5tsLD5rCK",
    "outputId": "52617138-a686-49a7-c489-4dc65b5caad7"
   },
   "outputs": [],
   "source": [
    "env_id = env_id_1\n",
    "test_env = gen_wrapped_env(env_id)\n",
    "# generate a random initialization for the environment\n",
    "\n",
    "test_env.seed(randint(1, num_cpu))\n",
    "observation = test_env.reset()\n",
    "\n",
    "done = False\n",
    "episode_reward = 0\n",
    "episode_length = 0\n",
    "\n",
    "while not done:\n",
    "  #test_env.render()\n",
    "  action, states = model.predict(observation, deterministic=False)\n",
    "  observation, reward, done, info = test_env.step(action)\n",
    "  episode_reward += reward\n",
    "  episode_length += 1\n",
    "\n",
    "print('Total reward:', episode_reward)\n",
    "print('Total length:', episode_length)\n",
    "\n",
    "test_env.close()\n",
    "show_video()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n0sfVEmCrbSl"
   },
   "source": [
    "## Learn second environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IhR_CogRrbSn"
   },
   "outputs": [],
   "source": [
    "#model = PPO.load(path=tb_log_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6X50nqSArbSl"
   },
   "outputs": [],
   "source": [
    "model.set_env(vec_env_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mLksPvHErbSl"
   },
   "outputs": [],
   "source": [
    "log_dir = \"./tmp/gym/\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "tb_log_name = 'task02-lava'\n",
    "\n",
    "# Create eval environment\n",
    "env = gym.make(env_id_2)\n",
    "env = FlatObsWrapper(env)\n",
    "eval_env = stable_baselines3.common.monitor.Monitor(env, log_dir)\n",
    "# Reset the environment\n",
    "eval_env.reset();\n",
    "\n",
    "# Stop training when the model reaches the reward threshold\n",
    "callback_on_best = StopTrainingOnRewardThreshold(reward_threshold=0.92, verbose=1)\n",
    "eval_callback = EvalCallback(eval_env, log_path=log_dir, callback_on_new_best=callback_on_best, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AaydIunmrbSm",
    "outputId": "5af05be1-5ff5-4e61-8059-df4c46ab4bab"
   },
   "outputs": [],
   "source": [
    "# number of timesteps to add\n",
    "total_timesteps = 5000000\n",
    "log_interval = 10\n",
    "\n",
    "# resume training using same tensorboard run without resetting the timesteps\n",
    "model.learn(total_timesteps=total_timesteps,\n",
    "            log_interval=log_interval,\n",
    "            tb_log_name=tb_log_name,\n",
    "            reset_num_timesteps=True,\n",
    "            callback=eval_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GpPwz9fGrbSm",
    "outputId": "73d281b0-a032-4592-b2c8-ae322d5095e2"
   },
   "outputs": [],
   "source": [
    "print(tb_log_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "krj7XuahrbSm"
   },
   "outputs": [],
   "source": [
    "# Save the agent\n",
    "model.save(tb_log_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HBowudtCrbSn",
    "outputId": "f769b0b6-742d-467d-e3fc-44b39fc98604"
   },
   "outputs": [],
   "source": [
    "# We create a separate environment for evaluation\n",
    "eval_env = FlatObsWrapper(gym.make(env_id_2))\n",
    "eval_env.seed(10000+randint(0, 10))\n",
    "eval_env.reset()\n",
    "# Random Agent, before training\n",
    "mean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=20)\n",
    "print(f'Mean reward: {mean_reward} +/- {std_reward:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FpW4ondMrbSo",
    "outputId": "60ea5f5b-2097-42cb-e0d4-c1bd04b9df36"
   },
   "outputs": [],
   "source": [
    "test_env = gen_wrapped_env(env_id_2)\n",
    "observation = test_env.reset()\n",
    "\n",
    "done = False\n",
    "episode_reward = 0\n",
    "episode_length = 0\n",
    "\n",
    "while not done:\n",
    "  #test_env.render()\n",
    "  action, states = model.predict(observation, deterministic=False)\n",
    "  observation, reward, done, info = test_env.step(action)\n",
    "  episode_reward += reward\n",
    "  episode_length += 1\n",
    "\n",
    "print('Total reward:', episode_reward)\n",
    "print('Total length:', episode_length)\n",
    "\n",
    "test_env.close()\n",
    "show_video()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-psMgyOBrbSo",
    "outputId": "1f570f75-744e-47e2-cfb3-d8654ee4c541"
   },
   "outputs": [],
   "source": [
    "# We create a separate environment for evaluation\n",
    "eval_env = FlatObsWrapper(gym.make(env_id_1))\n",
    "eval_env.seed(10000+randint(0, 10))\n",
    "eval_env.reset()\n",
    "# Random Agent, before training\n",
    "mean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=20)\n",
    "print(f'Mean reward: {mean_reward} +/- {std_reward:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s25OB5OKrbSo",
    "outputId": "b115e8ea-ecd2-4eeb-dc61-94fd59819f7a"
   },
   "outputs": [],
   "source": [
    "test_env = gen_wrapped_env(env_id_1)\n",
    "test_env.seed(10000+randint(0, 10))\n",
    "observation = test_env.reset()\n",
    "\n",
    "done = False\n",
    "episode_reward = 0\n",
    "episode_length = 0\n",
    "\n",
    "while not done:\n",
    "  #test_env.render()\n",
    "  action, states = model.predict(observation, deterministic=False)\n",
    "  observation, reward, done, info = test_env.step(action)\n",
    "  episode_reward += reward\n",
    "  episode_length += 1\n",
    "\n",
    "print('Total reward:', episode_reward)\n",
    "print('Total length:', episode_length)\n",
    "\n",
    "test_env.close()\n",
    "show_video()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RFTcnLV7rbSp"
   },
   "source": [
    "# Test training with CNN Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By default, we use a DummyVecEnv as it is usually faster (cf doc)\n",
    "num_cpu = 16  # Number of processes to use\n",
    "\n",
    "env_id_1 = 'MiniGrid-DoorKey-6x6-v0'\n",
    "vec_env_1 = make_vec_env(env_id_1, n_envs=num_cpu, wrapper_class=ImgRGBImgPartialObsWrapper, seed=10000)\n",
    "\n",
    "env_id_2 = 'MiniGrid-LavaGapS6-v0'\n",
    "vec_env_2 = make_vec_env(env_id_2, n_envs=num_cpu, wrapper_class=ImgRGBImgPartialObsWrapper, seed=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test which device is in use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from stable_baselines3.common.utils import get_device\n",
    "get_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test wrapper output shape:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation: [[[[55 55 55]\n",
      "   [33 33 33]\n",
      "   [33 33 33]\n",
      "   ...\n",
      "   [33 33 33]\n",
      "   [33 33 33]\n",
      "   [33 33 33]]\n",
      "\n",
      "  [[33 33 33]\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]\n",
      "   ...\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]]\n",
      "\n",
      "  [[33 33 33]\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]\n",
      "   ...\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[99 99 99]\n",
      "   [76 76 76]\n",
      "   [76 76 76]\n",
      "   ...\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]]\n",
      "\n",
      "  [[99 99 99]\n",
      "   [76 76 76]\n",
      "   [76 76 76]\n",
      "   ...\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]]\n",
      "\n",
      "  [[99 99 99]\n",
      "   [76 76 76]\n",
      "   [76 76 76]\n",
      "   ...\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]]]\n",
      "\n",
      "\n",
      " [[[55 55 55]\n",
      "   [33 33 33]\n",
      "   [33 33 33]\n",
      "   ...\n",
      "   [33 33 33]\n",
      "   [33 33 33]\n",
      "   [33 33 33]]\n",
      "\n",
      "  [[33 33 33]\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]\n",
      "   ...\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]]\n",
      "\n",
      "  [[33 33 33]\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]\n",
      "   ...\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[33 33 33]\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]\n",
      "   ...\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]]\n",
      "\n",
      "  [[33 33 33]\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]\n",
      "   ...\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]]\n",
      "\n",
      "  [[33 33 33]\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]\n",
      "   ...\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]]]\n",
      "\n",
      "\n",
      " [[[55 55 55]\n",
      "   [33 33 33]\n",
      "   [33 33 33]\n",
      "   ...\n",
      "   [33 33 33]\n",
      "   [33 33 33]\n",
      "   [33 33 33]]\n",
      "\n",
      "  [[33 33 33]\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]\n",
      "   ...\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]]\n",
      "\n",
      "  [[33 33 33]\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]\n",
      "   ...\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[99 99 99]\n",
      "   [76 76 76]\n",
      "   [76 76 76]\n",
      "   ...\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]]\n",
      "\n",
      "  [[99 99 99]\n",
      "   [76 76 76]\n",
      "   [76 76 76]\n",
      "   ...\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]]\n",
      "\n",
      "  [[99 99 99]\n",
      "   [76 76 76]\n",
      "   [76 76 76]\n",
      "   ...\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[55 55 55]\n",
      "   [33 33 33]\n",
      "   [33 33 33]\n",
      "   ...\n",
      "   [33 33 33]\n",
      "   [33 33 33]\n",
      "   [33 33 33]]\n",
      "\n",
      "  [[33 33 33]\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]\n",
      "   ...\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]]\n",
      "\n",
      "  [[33 33 33]\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]\n",
      "   ...\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[33 33 33]\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]\n",
      "   ...\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]]\n",
      "\n",
      "  [[33 33 33]\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]\n",
      "   ...\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]]\n",
      "\n",
      "  [[33 33 33]\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]\n",
      "   ...\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]]]\n",
      "\n",
      "\n",
      " [[[55 55 55]\n",
      "   [33 33 33]\n",
      "   [33 33 33]\n",
      "   ...\n",
      "   [33 33 33]\n",
      "   [33 33 33]\n",
      "   [33 33 33]]\n",
      "\n",
      "  [[33 33 33]\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]\n",
      "   ...\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]]\n",
      "\n",
      "  [[33 33 33]\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]\n",
      "   ...\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[99 99 99]\n",
      "   [76 76 76]\n",
      "   [76 76 76]\n",
      "   ...\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]]\n",
      "\n",
      "  [[99 99 99]\n",
      "   [76 76 76]\n",
      "   [76 76 76]\n",
      "   ...\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]]\n",
      "\n",
      "  [[99 99 99]\n",
      "   [76 76 76]\n",
      "   [76 76 76]\n",
      "   ...\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]]]\n",
      "\n",
      "\n",
      " [[[55 55 55]\n",
      "   [33 33 33]\n",
      "   [33 33 33]\n",
      "   ...\n",
      "   [33 33 33]\n",
      "   [33 33 33]\n",
      "   [33 33 33]]\n",
      "\n",
      "  [[33 33 33]\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]\n",
      "   ...\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]]\n",
      "\n",
      "  [[33 33 33]\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]\n",
      "   ...\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[33 33 33]\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]\n",
      "   ...\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]]\n",
      "\n",
      "  [[33 33 33]\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]\n",
      "   ...\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]]\n",
      "\n",
      "  [[33 33 33]\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]\n",
      "   ...\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]]]] , Observation Shape:  (16, 56, 56, 3)\n"
     ]
    }
   ],
   "source": [
    "obs = vec_env_1.reset()\n",
    "print('Observation:', obs, ', Observation Shape: ', obs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.00005\n",
    "n_steps = 256\n",
    "batch_size = 16\n",
    "ent_coef = 0.001\n",
    "n_epochs = 4\n",
    "\n",
    "tensorboard_log = \"./tmp/log/\"\n",
    "os.makedirs(tensorboard_log, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learn first environment with CnnPolicy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env in a VecTransposeImage.\n"
     ]
    }
   ],
   "source": [
    "# Reset the environment\n",
    "vec_env_1.reset()\n",
    "\n",
    "# create the model\n",
    "model = PPO('CnnPolicy',\n",
    "            env=vec_env_1,\n",
    "            learning_rate=learning_rate,\n",
    "            batch_size=batch_size,\n",
    "            ent_coef=ent_coef,\n",
    "            n_epochs=n_epochs,\n",
    "            n_steps=n_steps,\n",
    "            tensorboard_log=tensorboard_log,\n",
    "            verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"./tmp/gym/\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "tb_log_name = 'task01-doorkey-6x6_cnn'\n",
    "\n",
    "# Create eval environment\n",
    "env = gym.make(env_id_1)\n",
    "env = ImgRGBImgPartialObsWrapper(env)\n",
    "eval_env = stable_baselines3.common.monitor.Monitor(env, log_dir)\n",
    "# Reset the environment\n",
    "eval_env.reset();\n",
    "\n",
    "# Stop training when the model reaches the reward threshold\n",
    "callback_on_best = StopTrainingOnRewardThreshold(reward_threshold=0.92, verbose=1)\n",
    "eval_callback = EvalCallback(eval_env, log_path=log_dir, callback_on_new_best=callback_on_best, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_timesteps = 1000000\n",
    "log_interval = 10\n",
    "\n",
    "model.learn(total_timesteps=total_timesteps,\n",
    "            log_interval=log_interval,\n",
    "            tb_log_name = tb_log_name,\n",
    "            callback=eval_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the agent\n",
    "model.save(tb_log_name)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "test_minigrid_sb3_curriculum.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "51a1b245ec7fc72d2061587b663ba1f584a95fe24cf1a61a943e144e8f966db4"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
