{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MNF5-qJ7B0Q3"
   },
   "source": [
    "# Testing Stable Baselines3 with gym-MiniGrid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AdM2FnEJB0Q8"
   },
   "source": [
    "## Basic Jupyter Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 319,
     "status": "ok",
     "timestamp": 1647123362972,
     "user": {
      "displayName": "I単igo",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "14378798962183195551"
     },
     "user_tz": -60
    },
    "id": "aycUmr6OB0Q8"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pprint import pprint\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ef0DdE0b4pLd"
   },
   "source": [
    "## Import Stable-Baselines3 and initial set-up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PLM4YYcL5rBt"
   },
   "source": [
    "Import libraries and classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YgenDMtf4pLe"
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import time\n",
    "import io\n",
    "import os\n",
    "import glob\n",
    "import torch\n",
    "import base64\n",
    "import stable_baselines3\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from random import randint \n",
    "from stable_baselines3 import PPO, DQN, A2C\n",
    "from stable_baselines3.common.results_plotter import ts2xy, load_results\n",
    "from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnRewardThreshold\n",
    "from stable_baselines3.common.env_util import make_atari_env\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.utils import set_random_seed\n",
    "from stable_baselines3.common.vec_env import SubprocVecEnv, DummyVecEnv\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "import gym\n",
    "from gym import spaces\n",
    "from gym.wrappers import Monitor\n",
    "import gym_minigrid\n",
    "from gym_minigrid.wrappers import FlatObsWrapper, ImgObsWrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "28U_WEp25rBu"
   },
   "source": [
    "Define the video function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d7eCH8Kf4pLf"
   },
   "outputs": [],
   "source": [
    "import base64\n",
    "import glob\n",
    "import io\n",
    "from IPython.display import HTML\n",
    "from IPython import display \n",
    "\n",
    "def show_video():\n",
    "    mp4list = glob.glob('video/*.mp4')\n",
    "    if len(mp4list) > 0:\n",
    "        mp4 = mp4list[0]\n",
    "        video = io.open(mp4, 'r+b').read()\n",
    "        encoded = base64.b64encode(video)\n",
    "        display.display(HTML(data='''<video alt=\"test\" autoplay \n",
    "                loop controls style=\"height: 400px;\">\n",
    "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
    "             </video>'''.format(encoded.decode('ascii'))))\n",
    "    else:\n",
    "        print(\"Could not find video\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KchGuXpd5rBv"
   },
   "source": [
    "Define the rendering wrappers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gdhk3Oep4pLf"
   },
   "outputs": [],
   "source": [
    "from gym.wrappers import Monitor\n",
    "\n",
    "# Monitor is a gym wrapper, which helps easy rendering of videos of the wrapped environment.\n",
    "def wrap_env(env):\n",
    "    env = Monitor(env, './video', force=True)\n",
    "    return env\n",
    "\n",
    "def gen_wrapped_env(env_name):\n",
    "    return wrap_env(FlatObsWrapper(gym.make(env_name)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "executionInfo": {
     "elapsed": 900,
     "status": "ok",
     "timestamp": 1647083269049,
     "user": {
      "displayName": "I単igo",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "14378798962183195551"
     },
     "user_tz": -60
    },
    "id": "B21JwGYn5rBv",
    "outputId": "54dfa526-2621-4f91-df2b-e4ff7a447aad"
   },
   "outputs": [],
   "source": [
    "from random import randint\n",
    "#env_id = 'MiniGrid-Empty-16x16-v0'\n",
    "env_id = 'MiniGrid-DoorKey-6x6-v0'\n",
    "#env_id = 'MiniGrid-DistShift1-v0'\n",
    "#env_id ='MiniGrid-UnlockPickup-v0'\n",
    "#env_id = 'MiniGrid-RedBlueDoors-6x6-v0'\n",
    "#env_id = 'MiniGrid-LavaGapS6-v0'\n",
    "\n",
    "eval_env = gym.make(env_id)\n",
    "eval_env.seed(10000+randint(0, 10))\n",
    "eval_env.reset()\n",
    "before_img = eval_env.render('rgb_array')\n",
    "\n",
    "plt.imshow(before_img);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "umY09KJP5rCI"
   },
   "source": [
    "# Catastrophic Forgetting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yPq1XkeL5rCI"
   },
   "source": [
    "## Define the environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eYzbUoTS5rCI"
   },
   "outputs": [],
   "source": [
    "# By default, we use a DummyVecEnv as it is usually faster (cf doc)\n",
    "num_cpu = 16  # Number of processes to use\n",
    "\n",
    "env_id_1 = 'MiniGrid-DoorKey-6x6-v0'\n",
    "vec_env_1 = make_vec_env(env_id_1, n_envs=num_cpu, wrapper_class=FlatObsWrapper, seed=10000)\n",
    "\n",
    "env_id_2 = 'MiniGrid-LavaGapS6-v0'\n",
    "vec_env_2 = make_vec_env(env_id_2, n_envs=num_cpu, wrapper_class=FlatObsWrapper, seed=5000)\n",
    "\n",
    "env_id_3 = 'MiniGrid-RedBlueDoors-6x6-v0'\n",
    "vec_env_3 = make_vec_env(env_id_3, n_envs=num_cpu, wrapper_class=FlatObsWrapper, seed=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 197,
     "status": "ok",
     "timestamp": 1647085386489,
     "user": {
      "displayName": "I単igo",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "14378798962183195551"
     },
     "user_tz": -60
    },
    "id": "vT24ysNm79CE",
    "outputId": "8f710f58-57f4-44c7-aebf-d24c88c99da3"
   },
   "outputs": [],
   "source": [
    "from stable_baselines3.common.utils import get_device\n",
    "get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "94zAf0-I5rCI"
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.00005\n",
    "n_steps = 256\n",
    "batch_size = 16\n",
    "ent_coef = 0.001\n",
    "n_epochs = 4\n",
    "\n",
    "tensorboard_log = \"./tmp/log/\"\n",
    "os.makedirs(tensorboard_log, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SDhlDj-J5rCI"
   },
   "source": [
    "## Learn first environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9700,
     "status": "ok",
     "timestamp": 1646782989154,
     "user": {
      "displayName": "I単igo",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "14378798962183195551"
     },
     "user_tz": -60
    },
    "id": "fgNWgfba5rCI",
    "outputId": "553d07ab-f3db-4c45-e45a-b2624d78a477",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Reset the environment\n",
    "vec_env_1.reset()\n",
    "\n",
    "# create the model\n",
    "model = PPO('MlpPolicy',\n",
    "            env=vec_env_1,\n",
    "            learning_rate=learning_rate,\n",
    "            batch_size=batch_size,\n",
    "            ent_coef=ent_coef,\n",
    "            n_epochs=n_epochs,\n",
    "            n_steps=n_steps,\n",
    "            tensorboard_log=tensorboard_log,\n",
    "            verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a75Hyh4n5rCJ"
   },
   "outputs": [],
   "source": [
    "log_dir = \"./tmp/gym/\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "tb_log_name = 'task01-doorkey-6x6'\n",
    "\n",
    "# Create eval environment\n",
    "env = gym.make(env_id_1)\n",
    "env = FlatObsWrapper(env)\n",
    "eval_env = stable_baselines3.common.monitor.Monitor(env, log_dir)\n",
    "# Reset the environment\n",
    "eval_env.reset();\n",
    "\n",
    "# Stop training when the model reaches the reward threshold\n",
    "callback_on_best = StopTrainingOnRewardThreshold(reward_threshold=0.92, verbose=1)\n",
    "eval_callback = EvalCallback(eval_env, log_path=log_dir, callback_on_new_best=callback_on_best, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cBedyr7x5rCJ",
    "outputId": "77956ad4-b758-4779-c8d2-3267555ba0b6",
    "tags": []
   },
   "outputs": [],
   "source": [
    "total_timesteps = 1000000\n",
    "log_interval = 10\n",
    "\n",
    "model.learn(total_timesteps=total_timesteps,\n",
    "            log_interval=log_interval,\n",
    "            tb_log_name = tb_log_name,\n",
    "            callback=eval_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DO-9hK8frbSj",
    "outputId": "772b6bd5-3a8e-40a9-bb47-a1bf792b7fdb"
   },
   "outputs": [],
   "source": [
    "print(tb_log_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4qWDyB355rCJ"
   },
   "outputs": [],
   "source": [
    "# Save the agent\n",
    "model.save(tb_log_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XtIhYAxd5rCJ"
   },
   "outputs": [],
   "source": [
    "#model = PPO.load(path=tb_log_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iz6ij9Nt5rCJ",
    "outputId": "85503a8d-7a79-493b-d06f-bd9ccbb0dc83"
   },
   "outputs": [],
   "source": [
    "# We create a separate environment for evaluation\n",
    "eval_env = FlatObsWrapper(gym.make(env_id_1))\n",
    "\n",
    "# Random Agent, before training\n",
    "mean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=20)\n",
    "print(f'Mean reward: {mean_reward} +/- {std_reward:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3qj5tsLD5rCK",
    "outputId": "52617138-a686-49a7-c489-4dc65b5caad7"
   },
   "outputs": [],
   "source": [
    "env_id = env_id_1\n",
    "test_env = gen_wrapped_env(env_id)\n",
    "# generate a random initialization for the environment\n",
    "\n",
    "test_env.seed(randint(1, num_cpu))\n",
    "observation = test_env.reset()\n",
    "\n",
    "done = False\n",
    "episode_reward = 0\n",
    "episode_length = 0\n",
    "\n",
    "while not done:\n",
    "  #test_env.render()\n",
    "  action, states = model.predict(observation, deterministic=False)\n",
    "  observation, reward, done, info = test_env.step(action)\n",
    "  episode_reward += reward\n",
    "  episode_length += 1\n",
    "\n",
    "print('Total reward:', episode_reward)\n",
    "print('Total length:', episode_length)\n",
    "\n",
    "test_env.close()\n",
    "show_video()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n0sfVEmCrbSl"
   },
   "source": [
    "## Learn second environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IhR_CogRrbSn"
   },
   "outputs": [],
   "source": [
    "#model = PPO.load(path=tb_log_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6X50nqSArbSl"
   },
   "outputs": [],
   "source": [
    "model.set_env(vec_env_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mLksPvHErbSl"
   },
   "outputs": [],
   "source": [
    "log_dir = \"./tmp/gym/\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "tb_log_name = 'task02-lava'\n",
    "\n",
    "# Create eval environment\n",
    "env = gym.make(env_id_2)\n",
    "env = FlatObsWrapper(env)\n",
    "eval_env = stable_baselines3.common.monitor.Monitor(env, log_dir)\n",
    "# Reset the environment\n",
    "eval_env.reset();\n",
    "\n",
    "# Stop training when the model reaches the reward threshold\n",
    "callback_on_best = StopTrainingOnRewardThreshold(reward_threshold=0.92, verbose=1)\n",
    "eval_callback = EvalCallback(eval_env, log_path=log_dir, callback_on_new_best=callback_on_best, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AaydIunmrbSm",
    "outputId": "5af05be1-5ff5-4e61-8059-df4c46ab4bab"
   },
   "outputs": [],
   "source": [
    "# number of timesteps to add\n",
    "total_timesteps = 5000000\n",
    "log_interval = 10\n",
    "\n",
    "# resume training using same tensorboard run without resetting the timesteps\n",
    "model.learn(total_timesteps=total_timesteps,\n",
    "            log_interval=log_interval,\n",
    "            tb_log_name=tb_log_name,\n",
    "            reset_num_timesteps=True,\n",
    "            callback=eval_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GpPwz9fGrbSm",
    "outputId": "73d281b0-a032-4592-b2c8-ae322d5095e2"
   },
   "outputs": [],
   "source": [
    "print(tb_log_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "krj7XuahrbSm"
   },
   "outputs": [],
   "source": [
    "# Save the agent\n",
    "model.save(tb_log_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HBowudtCrbSn",
    "outputId": "f769b0b6-742d-467d-e3fc-44b39fc98604"
   },
   "outputs": [],
   "source": [
    "# We create a separate environment for evaluation\n",
    "eval_env = FlatObsWrapper(gym.make(env_id_2))\n",
    "eval_env.seed(10000+randint(0, 10))\n",
    "eval_env.reset()\n",
    "# Random Agent, before training\n",
    "mean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=20)\n",
    "print(f'Mean reward: {mean_reward} +/- {std_reward:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FpW4ondMrbSo",
    "outputId": "60ea5f5b-2097-42cb-e0d4-c1bd04b9df36"
   },
   "outputs": [],
   "source": [
    "test_env = gen_wrapped_env(env_id_2)\n",
    "observation = test_env.reset()\n",
    "\n",
    "done = False\n",
    "episode_reward = 0\n",
    "episode_length = 0\n",
    "\n",
    "while not done:\n",
    "  #test_env.render()\n",
    "  action, states = model.predict(observation, deterministic=False)\n",
    "  observation, reward, done, info = test_env.step(action)\n",
    "  episode_reward += reward\n",
    "  episode_length += 1\n",
    "\n",
    "print('Total reward:', episode_reward)\n",
    "print('Total length:', episode_length)\n",
    "\n",
    "test_env.close()\n",
    "show_video()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-psMgyOBrbSo",
    "outputId": "1f570f75-744e-47e2-cfb3-d8654ee4c541"
   },
   "outputs": [],
   "source": [
    "# We create a separate environment for evaluation\n",
    "eval_env = FlatObsWrapper(gym.make(env_id_1))\n",
    "eval_env.seed(10000+randint(0, 10))\n",
    "eval_env.reset()\n",
    "# Random Agent, before training\n",
    "mean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=20)\n",
    "print(f'Mean reward: {mean_reward} +/- {std_reward:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s25OB5OKrbSo",
    "outputId": "b115e8ea-ecd2-4eeb-dc61-94fd59819f7a"
   },
   "outputs": [],
   "source": [
    "test_env = gen_wrapped_env(env_id_1)\n",
    "test_env.seed(10000+randint(0, 10))\n",
    "observation = test_env.reset()\n",
    "\n",
    "done = False\n",
    "episode_reward = 0\n",
    "episode_length = 0\n",
    "\n",
    "while not done:\n",
    "  #test_env.render()\n",
    "  action, states = model.predict(observation, deterministic=False)\n",
    "  observation, reward, done, info = test_env.step(action)\n",
    "  episode_reward += reward\n",
    "  episode_length += 1\n",
    "\n",
    "print('Total reward:', episode_reward)\n",
    "print('Total length:', episode_length)\n",
    "\n",
    "test_env.close()\n",
    "show_video()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RFTcnLV7rbSp"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "test_minigrid_sb3_curriculum.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "51a1b245ec7fc72d2061587b663ba1f584a95fe24cf1a61a943e144e8f966db4"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
