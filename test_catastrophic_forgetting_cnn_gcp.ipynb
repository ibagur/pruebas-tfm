{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7178980a",
   "metadata": {
    "id": "MNF5-qJ7B0Q3"
   },
   "source": [
    "# Testing Catastrophic forgetting with gym-MiniGrid trained over CNN policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79efbe7f",
   "metadata": {
    "id": "AdM2FnEJB0Q8"
   },
   "source": [
    "## Basic Jupyter Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3bbbdd2c",
   "metadata": {
    "collapsed": false,
    "executionInfo": {
     "elapsed": 319,
     "status": "ok",
     "timestamp": 1647123362972,
     "user": {
      "displayName": "IÃ±igo",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "14378798962183195551"
     },
     "user_tz": -60
    },
    "id": "aycUmr6OB0Q8",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pprint import pprint\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1517c0f4",
   "metadata": {
    "id": "ef0DdE0b4pLd"
   },
   "source": [
    "## Import Stable-Baselines3 and initial set-up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb356eb",
   "metadata": {
    "id": "PLM4YYcL5rBt"
   },
   "source": [
    "Import libraries and classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1f1d680",
   "metadata": {
    "collapsed": false,
    "id": "YgenDMtf4pLe",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import time\n",
    "import io\n",
    "import os\n",
    "import glob\n",
    "import torch\n",
    "import base64\n",
    "import stable_baselines3\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from random import randint \n",
    "from stable_baselines3 import PPO, DQN, A2C\n",
    "from stable_baselines3.common.results_plotter import ts2xy, load_results\n",
    "from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnRewardThreshold\n",
    "from stable_baselines3.common.env_util import make_atari_env\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.utils import set_random_seed\n",
    "from stable_baselines3.common.vec_env import SubprocVecEnv, DummyVecEnv\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "import gym\n",
    "from gym import spaces\n",
    "from gym.wrappers import Monitor\n",
    "import gym_minigrid\n",
    "from gym_minigrid.wrappers import FlatObsWrapper, ImgObsWrapper, RGBImgPartialObsWrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5021d56b",
   "metadata": {},
   "source": [
    "Define wrapper for CNN Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17b2fa8e",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def ImgRGBImgPartialObsWrapper(env):\n",
    "    return ImgObsWrapper(RGBImgPartialObsWrapper(env))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44dc8005",
   "metadata": {
    "id": "28U_WEp25rBu"
   },
   "source": [
    "Define the video function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "399510b1",
   "metadata": {
    "collapsed": false,
    "id": "d7eCH8Kf4pLf",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import base64\n",
    "import glob\n",
    "import io\n",
    "from IPython.display import HTML\n",
    "from IPython import display \n",
    "\n",
    "def show_video():\n",
    "    mp4list = glob.glob('video/*.mp4')\n",
    "    if len(mp4list) > 0:\n",
    "        mp4 = mp4list[0]\n",
    "        video = io.open(mp4, 'r+b').read()\n",
    "        encoded = base64.b64encode(video)\n",
    "        display.display(HTML(data='''<video alt=\"test\" autoplay \n",
    "                loop controls style=\"height: 400px;\">\n",
    "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
    "             </video>'''.format(encoded.decode('ascii'))))\n",
    "    else:\n",
    "        print(\"Could not find video\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655fcddc",
   "metadata": {
    "id": "KchGuXpd5rBv"
   },
   "source": [
    "Define the rendering wrappers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5133c66",
   "metadata": {
    "collapsed": false,
    "id": "Gdhk3Oep4pLf",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from gym.wrappers import Monitor\n",
    "\n",
    "# Monitor is a gym wrapper, which helps easy rendering of videos of the wrapped environment.\n",
    "def wrap_env(env):\n",
    "    env = Monitor(env, './video', force=True)\n",
    "    return env\n",
    "\n",
    "def gen_wrapped_env(env_name):\n",
    "    return wrap_env(FlatObsWrapper(gym.make(env_name)))\n",
    "\n",
    "def gen_wrapped_env_cnn(env_name):\n",
    "    return wrap_env(ImgObsWrapper(RGBImgPartialObsWrapper(gym.make(env_name))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb3aa6f",
   "metadata": {
    "id": "RFTcnLV7rbSp"
   },
   "source": [
    "# Test training with CNN Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2dcb6f",
   "metadata": {},
   "source": [
    "## Define the environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6cb767c7",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# By default, we use a DummyVecEnv as it is usually faster (cf doc)\n",
    "num_cpu = 16  # Number of processes to use\n",
    "\n",
    "env_id_1 = 'MiniGrid-DoorKey-6x6-v0'\n",
    "vec_env_1 = make_vec_env(env_id_1, n_envs=num_cpu, wrapper_class=ImgRGBImgPartialObsWrapper, seed=10000)\n",
    "\n",
    "env_id_2 = 'MiniGrid-LavaGapS6-v0'\n",
    "vec_env_2 = make_vec_env(env_id_2, n_envs=num_cpu, wrapper_class=ImgRGBImgPartialObsWrapper, seed=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62b8940",
   "metadata": {},
   "source": [
    "Test which device is in use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1cd85af9",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from stable_baselines3.common.utils import get_device\n",
    "get_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22061fe",
   "metadata": {},
   "source": [
    "Test wrapper output shape:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d4ad2c80",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.00005\n",
    "n_steps = 256\n",
    "batch_size = 16\n",
    "ent_coef = 0.001\n",
    "n_epochs = 4\n",
    "\n",
    "tensorboard_log = \"./tmp/log/\"\n",
    "os.makedirs(tensorboard_log, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a13752",
   "metadata": {},
   "source": [
    "## Learn first environment with CNN Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f244602f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env in a VecTransposeImage.\n"
     ]
    }
   ],
   "source": [
    "# Reset the environment\n",
    "vec_env_1.reset()\n",
    "\n",
    "# create the model\n",
    "model = PPO('CnnPolicy',\n",
    "            env=vec_env_1,\n",
    "            learning_rate=learning_rate,\n",
    "            batch_size=batch_size,\n",
    "            ent_coef=ent_coef,\n",
    "            n_epochs=n_epochs,\n",
    "            n_steps=n_steps,\n",
    "            tensorboard_log=tensorboard_log,\n",
    "            verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9cd14638",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "log_dir = \"./tmp/gym/\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "tb_log_name = 'task01-doorkey-6x6_cnn'\n",
    "\n",
    "# Create eval environment\n",
    "env = gym.make(env_id_1)\n",
    "env = ImgRGBImgPartialObsWrapper(env)\n",
    "eval_env = stable_baselines3.common.monitor.Monitor(env, log_dir)\n",
    "# Reset the environment\n",
    "eval_env.reset();\n",
    "\n",
    "# Stop training when the model reaches the reward threshold\n",
    "callback_on_best = StopTrainingOnRewardThreshold(reward_threshold=0.92, verbose=1)\n",
    "eval_callback = EvalCallback(eval_env, log_path=log_dir, callback_on_new_best=callback_on_best, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "40e6f23c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to ./tmp/log/task01-doorkey-6x6_cnn_3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/inigo/stable-baselines3/stable_baselines3/common/callbacks.py:345: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.vec_transpose.VecTransposeImage object at 0x7f0d48500f10> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x7f0ed0402ad0>\n",
      "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 355         |\n",
      "|    ep_rew_mean          | 0.0158      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 282         |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 144         |\n",
      "|    total_timesteps      | 40960       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009526908 |\n",
      "|    clip_fraction        | 0.0754      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.89       |\n",
      "|    explained_variance   | -0.709      |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0594     |\n",
      "|    n_updates            | 36          |\n",
      "|    policy_gradient_loss | -0.00487    |\n",
      "|    value_loss           | 0.000583    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 358         |\n",
      "|    ep_rew_mean          | 0.0062      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 274         |\n",
      "|    iterations           | 20          |\n",
      "|    time_elapsed         | 297         |\n",
      "|    total_timesteps      | 81920       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011165097 |\n",
      "|    clip_fraction        | 0.0981      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.86       |\n",
      "|    explained_variance   | -3.73       |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0281     |\n",
      "|    n_updates            | 76          |\n",
      "|    policy_gradient_loss | -0.00738    |\n",
      "|    value_loss           | 0.000267    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 347         |\n",
      "|    ep_rew_mean          | 0.039       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 274         |\n",
      "|    iterations           | 30          |\n",
      "|    time_elapsed         | 448         |\n",
      "|    total_timesteps      | 122880      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012094345 |\n",
      "|    clip_fraction        | 0.119       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.72       |\n",
      "|    explained_variance   | 0.058       |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0375     |\n",
      "|    n_updates            | 116         |\n",
      "|    policy_gradient_loss | -0.00624    |\n",
      "|    value_loss           | 0.0014      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=160000, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 360.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 360         |\n",
      "|    mean_reward          | 0           |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 160000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009561235 |\n",
      "|    clip_fraction        | 0.107       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.65       |\n",
      "|    explained_variance   | 0.274       |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | 0.0336      |\n",
      "|    n_updates            | 156         |\n",
      "|    policy_gradient_loss | -0.0062     |\n",
      "|    value_loss           | 0.00396     |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 320      |\n",
      "|    ep_rew_mean     | 0.126    |\n",
      "| time/              |          |\n",
      "|    fps             | 270      |\n",
      "|    iterations      | 40       |\n",
      "|    time_elapsed    | 605      |\n",
      "|    total_timesteps | 163840   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 242         |\n",
      "|    ep_rew_mean          | 0.355       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 270         |\n",
      "|    iterations           | 50          |\n",
      "|    time_elapsed         | 756         |\n",
      "|    total_timesteps      | 204800      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010056371 |\n",
      "|    clip_fraction        | 0.0975      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.41       |\n",
      "|    explained_variance   | 0.566       |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.00342    |\n",
      "|    n_updates            | 196         |\n",
      "|    policy_gradient_loss | -0.00283    |\n",
      "|    value_loss           | 0.00437     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 37.8        |\n",
      "|    ep_rew_mean          | 0.906       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 270         |\n",
      "|    iterations           | 60          |\n",
      "|    time_elapsed         | 907         |\n",
      "|    total_timesteps      | 245760      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016439907 |\n",
      "|    clip_fraction        | 0.19        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.14       |\n",
      "|    explained_variance   | 0.65        |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0457     |\n",
      "|    n_updates            | 236         |\n",
      "|    policy_gradient_loss | -0.0135     |\n",
      "|    value_loss           | 0.0102      |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 21.4       |\n",
      "|    ep_rew_mean          | 0.945      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 270        |\n",
      "|    iterations           | 70         |\n",
      "|    time_elapsed         | 1060       |\n",
      "|    total_timesteps      | 286720     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04242312 |\n",
      "|    clip_fraction        | 0.18       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.492     |\n",
      "|    explained_variance   | 0.745      |\n",
      "|    learning_rate        | 5e-05      |\n",
      "|    loss                 | -0.0631    |\n",
      "|    n_updates            | 276        |\n",
      "|    policy_gradient_loss | -0.00199   |\n",
      "|    value_loss           | 0.00413    |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=320000, episode_reward=0.97 +/- 0.00\n",
      "Episode length: 13.40 +/- 1.62\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 13.4       |\n",
      "|    mean_reward          | 0.967      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 320000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04100219 |\n",
      "|    clip_fraction        | 0.222      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.429     |\n",
      "|    explained_variance   | 0.559      |\n",
      "|    learning_rate        | 5e-05      |\n",
      "|    loss                 | 0.0764     |\n",
      "|    n_updates            | 312        |\n",
      "|    policy_gradient_loss | -0.0131    |\n",
      "|    value_loss           | 0.00232    |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "Stopping training because the mean reward 0.97  is above the threshold 0.92\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x7f0d485536d0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_timesteps = 1000000\n",
    "log_interval = 10\n",
    "\n",
    "model.learn(total_timesteps=total_timesteps,\n",
    "            log_interval=log_interval,\n",
    "            tb_log_name = tb_log_name,\n",
    "            callback=eval_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e3c4fb51",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Save the agent\n",
    "model.save(tb_log_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "767921d3",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#model = PPO.load(path=tb_log_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cd7edcd0",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/inigo/stable-baselines3/stable_baselines3/common/evaluation.py:69: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  UserWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean reward: 0.866499987244606 +/- 0.29\n"
     ]
    }
   ],
   "source": [
    "# We create a separate environment for evaluation\n",
    "eval_env = ImgRGBImgPartialObsWrapper(gym.make(env_id_1))\n",
    "\n",
    "# Random Agent, before training\n",
    "mean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=20)\n",
    "print(f'Mean reward: {mean_reward} +/- {std_reward:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3b0810",
   "metadata": {},
   "source": [
    "## Learn second environment with CNN Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "30427f1f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env in a VecTransposeImage.\n"
     ]
    }
   ],
   "source": [
    "model.set_env(vec_env_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8190cea0",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "log_dir = \"./tmp/gym/\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "tb_log_name = 'task02-lava_cnn'\n",
    "\n",
    "# Create eval environment\n",
    "env = gym.make(env_id_1)\n",
    "env = ImgRGBImgPartialObsWrapper(env)\n",
    "eval_env = stable_baselines3.common.monitor.Monitor(env, log_dir)\n",
    "# Reset the environment\n",
    "eval_env.reset();\n",
    "\n",
    "# Stop training when the model reaches the reward threshold\n",
    "callback_on_best = StopTrainingOnRewardThreshold(reward_threshold=0.92, verbose=1)\n",
    "eval_callback = EvalCallback(eval_env, log_path=log_dir, callback_on_new_best=callback_on_best, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "173ca675",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to ./tmp/log/task02-lava_cnn_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/inigo/stable-baselines3/stable_baselines3/common/callbacks.py:345: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.vec_transpose.VecTransposeImage object at 0x7f0d44e31a10> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x7f0d4853f4d0>\n",
      "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 31.1        |\n",
      "|    ep_rew_mean          | 0.756       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 280         |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 146         |\n",
      "|    total_timesteps      | 40960       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025103115 |\n",
      "|    clip_fraction        | 0.234       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.96       |\n",
      "|    explained_variance   | 0.906       |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.061      |\n",
      "|    n_updates            | 348         |\n",
      "|    policy_gradient_loss | -0.0114     |\n",
      "|    value_loss           | 0.00614     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 10.8        |\n",
      "|    ep_rew_mean          | 0.933       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 272         |\n",
      "|    iterations           | 20          |\n",
      "|    time_elapsed         | 301         |\n",
      "|    total_timesteps      | 81920       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.034606304 |\n",
      "|    clip_fraction        | 0.21        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.49       |\n",
      "|    explained_variance   | 0.309       |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | 0.0493      |\n",
      "|    n_updates            | 388         |\n",
      "|    policy_gradient_loss | -0.0128     |\n",
      "|    value_loss           | 0.00693     |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 11.3       |\n",
      "|    ep_rew_mean          | 0.929      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 269        |\n",
      "|    iterations           | 30         |\n",
      "|    time_elapsed         | 456        |\n",
      "|    total_timesteps      | 122880     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02363367 |\n",
      "|    clip_fraction        | 0.177      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.569     |\n",
      "|    explained_variance   | 0.35       |\n",
      "|    learning_rate        | 5e-05      |\n",
      "|    loss                 | -0.0461    |\n",
      "|    n_updates            | 428        |\n",
      "|    policy_gradient_loss | -0.0121    |\n",
      "|    value_loss           | 0.013      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=160000, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 360.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 360        |\n",
      "|    mean_reward          | 0          |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 160000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04679626 |\n",
      "|    clip_fraction        | 0.175      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.437     |\n",
      "|    explained_variance   | 0.138      |\n",
      "|    learning_rate        | 5e-05      |\n",
      "|    loss                 | -0.0564    |\n",
      "|    n_updates            | 468        |\n",
      "|    policy_gradient_loss | -0.00639   |\n",
      "|    value_loss           | 0.00764    |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 10.6     |\n",
      "|    ep_rew_mean     | 0.934    |\n",
      "| time/              |          |\n",
      "|    fps             | 264      |\n",
      "|    iterations      | 40       |\n",
      "|    time_elapsed    | 618      |\n",
      "|    total_timesteps | 163840   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 9.95        |\n",
      "|    ep_rew_mean          | 0.938       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 264         |\n",
      "|    iterations           | 50          |\n",
      "|    time_elapsed         | 773         |\n",
      "|    total_timesteps      | 204800      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.035898563 |\n",
      "|    clip_fraction        | 0.247       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.556      |\n",
      "|    explained_variance   | 0.593       |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0751     |\n",
      "|    n_updates            | 508         |\n",
      "|    policy_gradient_loss | -0.0137     |\n",
      "|    value_loss           | 0.016       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 9.9         |\n",
      "|    ep_rew_mean          | 0.919       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 265         |\n",
      "|    iterations           | 60          |\n",
      "|    time_elapsed         | 927         |\n",
      "|    total_timesteps      | 245760      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.032356214 |\n",
      "|    clip_fraction        | 0.178       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.357      |\n",
      "|    explained_variance   | 0.2         |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0278     |\n",
      "|    n_updates            | 548         |\n",
      "|    policy_gradient_loss | -0.0116     |\n",
      "|    value_loss           | 0.00279     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 13          |\n",
      "|    ep_rew_mean          | 0.915       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 265         |\n",
      "|    iterations           | 70          |\n",
      "|    time_elapsed         | 1081        |\n",
      "|    total_timesteps      | 286720      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.033792444 |\n",
      "|    clip_fraction        | 0.154       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.239      |\n",
      "|    explained_variance   | 0.489       |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | 0.0778      |\n",
      "|    n_updates            | 588         |\n",
      "|    policy_gradient_loss | -0.00905    |\n",
      "|    value_loss           | 0.00225     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=320000, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 360.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 360        |\n",
      "|    mean_reward          | 0          |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 320000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08187314 |\n",
      "|    clip_fraction        | 0.256      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.444     |\n",
      "|    explained_variance   | 0.255      |\n",
      "|    learning_rate        | 5e-05      |\n",
      "|    loss                 | -0.019     |\n",
      "|    n_updates            | 624        |\n",
      "|    policy_gradient_loss | -0.00288   |\n",
      "|    value_loss           | 0.00295    |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 10.2        |\n",
      "|    ep_rew_mean          | 0.936       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 264         |\n",
      "|    iterations           | 80          |\n",
      "|    time_elapsed         | 1239        |\n",
      "|    total_timesteps      | 327680      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.037646137 |\n",
      "|    clip_fraction        | 0.241       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.547      |\n",
      "|    explained_variance   | 0.12        |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0341     |\n",
      "|    n_updates            | 628         |\n",
      "|    policy_gradient_loss | -0.0199     |\n",
      "|    value_loss           | 0.00441     |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 11.2       |\n",
      "|    ep_rew_mean          | 0.93       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 264        |\n",
      "|    iterations           | 90         |\n",
      "|    time_elapsed         | 1393       |\n",
      "|    total_timesteps      | 368640     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05735978 |\n",
      "|    clip_fraction        | 0.23       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.308     |\n",
      "|    explained_variance   | -0.0919    |\n",
      "|    learning_rate        | 5e-05      |\n",
      "|    loss                 | -0.0753    |\n",
      "|    n_updates            | 668        |\n",
      "|    policy_gradient_loss | -0.0169    |\n",
      "|    value_loss           | 0.000786   |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 10.2       |\n",
      "|    ep_rew_mean          | 0.927      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 264        |\n",
      "|    iterations           | 100        |\n",
      "|    time_elapsed         | 1548       |\n",
      "|    total_timesteps      | 409600     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03091572 |\n",
      "|    clip_fraction        | 0.227      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.392     |\n",
      "|    explained_variance   | 0.13       |\n",
      "|    learning_rate        | 5e-05      |\n",
      "|    loss                 | 0.0194     |\n",
      "|    n_updates            | 708        |\n",
      "|    policy_gradient_loss | -0.0265    |\n",
      "|    value_loss           | 0.00777    |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 14         |\n",
      "|    ep_rew_mean          | 0.911      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 264        |\n",
      "|    iterations           | 110        |\n",
      "|    time_elapsed         | 1704       |\n",
      "|    total_timesteps      | 450560     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05953094 |\n",
      "|    clip_fraction        | 0.268      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.606     |\n",
      "|    explained_variance   | 0.471      |\n",
      "|    learning_rate        | 5e-05      |\n",
      "|    loss                 | -0.0639    |\n",
      "|    n_updates            | 748        |\n",
      "|    policy_gradient_loss | -0.0122    |\n",
      "|    value_loss           | 0.0113     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=480000, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 360.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 360        |\n",
      "|    mean_reward          | 0          |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 480000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04917965 |\n",
      "|    clip_fraction        | 0.14       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.185     |\n",
      "|    explained_variance   | 0.638      |\n",
      "|    learning_rate        | 5e-05      |\n",
      "|    loss                 | -0.0661    |\n",
      "|    n_updates            | 780        |\n",
      "|    policy_gradient_loss | -0.00744   |\n",
      "|    value_loss           | 0.000568   |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 10.7        |\n",
      "|    ep_rew_mean          | 0.933       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 263         |\n",
      "|    iterations           | 120         |\n",
      "|    time_elapsed         | 1867        |\n",
      "|    total_timesteps      | 491520      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.090823606 |\n",
      "|    clip_fraction        | 0.274       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.354      |\n",
      "|    explained_variance   | 0.167       |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.034      |\n",
      "|    n_updates            | 788         |\n",
      "|    policy_gradient_loss | 0.0228      |\n",
      "|    value_loss           | 0.000475    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 9.1         |\n",
      "|    ep_rew_mean          | 0.943       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 262         |\n",
      "|    iterations           | 130         |\n",
      "|    time_elapsed         | 2025        |\n",
      "|    total_timesteps      | 532480      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.047082014 |\n",
      "|    clip_fraction        | 0.222       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.297      |\n",
      "|    explained_variance   | 0.535       |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0487     |\n",
      "|    n_updates            | 828         |\n",
      "|    policy_gradient_loss | 0.0227      |\n",
      "|    value_loss           | 0.003       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 10.9        |\n",
      "|    ep_rew_mean          | 0.922       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 262         |\n",
      "|    iterations           | 140         |\n",
      "|    time_elapsed         | 2181        |\n",
      "|    total_timesteps      | 573440      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.054709233 |\n",
      "|    clip_fraction        | 0.181       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.283      |\n",
      "|    explained_variance   | 0.608       |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0617     |\n",
      "|    n_updates            | 868         |\n",
      "|    policy_gradient_loss | 0.0116      |\n",
      "|    value_loss           | 0.00343     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 10.1        |\n",
      "|    ep_rew_mean          | 0.937       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 262         |\n",
      "|    iterations           | 150         |\n",
      "|    time_elapsed         | 2338        |\n",
      "|    total_timesteps      | 614400      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018853284 |\n",
      "|    clip_fraction        | 0.183       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.409      |\n",
      "|    explained_variance   | 0.41        |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0674     |\n",
      "|    n_updates            | 908         |\n",
      "|    policy_gradient_loss | -0.0198     |\n",
      "|    value_loss           | 0.00805     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=640000, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 360.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 360         |\n",
      "|    mean_reward          | 0           |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 640000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.059969693 |\n",
      "|    clip_fraction        | 0.196       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.219      |\n",
      "|    explained_variance   | 0.409       |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0168     |\n",
      "|    n_updates            | 936         |\n",
      "|    policy_gradient_loss | -0.0264     |\n",
      "|    value_loss           | 0.00127     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 9.08        |\n",
      "|    ep_rew_mean          | 0.943       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 262         |\n",
      "|    iterations           | 160         |\n",
      "|    time_elapsed         | 2500        |\n",
      "|    total_timesteps      | 655360      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024346305 |\n",
      "|    clip_fraction        | 0.0778      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.116      |\n",
      "|    explained_variance   | 0.494       |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0361     |\n",
      "|    n_updates            | 948         |\n",
      "|    policy_gradient_loss | -0.00789    |\n",
      "|    value_loss           | 0.000919    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 10.4        |\n",
      "|    ep_rew_mean          | 0.925       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 262         |\n",
      "|    iterations           | 170         |\n",
      "|    time_elapsed         | 2657        |\n",
      "|    total_timesteps      | 696320      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.033317856 |\n",
      "|    clip_fraction        | 0.188       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.387      |\n",
      "|    explained_variance   | 0.251       |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0804     |\n",
      "|    n_updates            | 988         |\n",
      "|    policy_gradient_loss | -0.017      |\n",
      "|    value_loss           | 0.00525     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 10.2        |\n",
      "|    ep_rew_mean          | 0.936       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 261         |\n",
      "|    iterations           | 180         |\n",
      "|    time_elapsed         | 2814        |\n",
      "|    total_timesteps      | 737280      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030784067 |\n",
      "|    clip_fraction        | 0.125       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.192      |\n",
      "|    explained_variance   | 0.507       |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.09       |\n",
      "|    n_updates            | 1028        |\n",
      "|    policy_gradient_loss | -0.0178     |\n",
      "|    value_loss           | 0.000745    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 9.04        |\n",
      "|    ep_rew_mean          | 0.943       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 261         |\n",
      "|    iterations           | 190         |\n",
      "|    time_elapsed         | 2970        |\n",
      "|    total_timesteps      | 778240      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.048357744 |\n",
      "|    clip_fraction        | 0.138       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.285      |\n",
      "|    explained_variance   | 0.0216      |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0554     |\n",
      "|    n_updates            | 1068        |\n",
      "|    policy_gradient_loss | -0.0139     |\n",
      "|    value_loss           | 0.0103      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=800000, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 360.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 360        |\n",
      "|    mean_reward          | 0          |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 800000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06455989 |\n",
      "|    clip_fraction        | 0.169      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.19      |\n",
      "|    explained_variance   | 0.327      |\n",
      "|    learning_rate        | 5e-05      |\n",
      "|    loss                 | 0.04       |\n",
      "|    n_updates            | 1092       |\n",
      "|    policy_gradient_loss | -0.00778   |\n",
      "|    value_loss           | 0.00205    |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 8.55        |\n",
      "|    ep_rew_mean          | 0.83        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 261         |\n",
      "|    iterations           | 200         |\n",
      "|    time_elapsed         | 3135        |\n",
      "|    total_timesteps      | 819200      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025019955 |\n",
      "|    clip_fraction        | 0.176       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.302      |\n",
      "|    explained_variance   | 0.628       |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0728     |\n",
      "|    n_updates            | 1108        |\n",
      "|    policy_gradient_loss | -0.0196     |\n",
      "|    value_loss           | 0.000683    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 8.51        |\n",
      "|    ep_rew_mean          | 0.947       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 261         |\n",
      "|    iterations           | 210         |\n",
      "|    time_elapsed         | 3290        |\n",
      "|    total_timesteps      | 860160      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.074910685 |\n",
      "|    clip_fraction        | 0.116       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.0966     |\n",
      "|    explained_variance   | 0.692       |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0231     |\n",
      "|    n_updates            | 1148        |\n",
      "|    policy_gradient_loss | -0.0147     |\n",
      "|    value_loss           | 0.00026     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 8.82        |\n",
      "|    ep_rew_mean          | 0.945       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 261         |\n",
      "|    iterations           | 220         |\n",
      "|    time_elapsed         | 3444        |\n",
      "|    total_timesteps      | 901120      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023628507 |\n",
      "|    clip_fraction        | 0.107       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.239      |\n",
      "|    explained_variance   | 0.52        |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.00463    |\n",
      "|    n_updates            | 1188        |\n",
      "|    policy_gradient_loss | -0.014      |\n",
      "|    value_loss           | 0.000653    |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 9.83       |\n",
      "|    ep_rew_mean          | 0.939      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 261        |\n",
      "|    iterations           | 230        |\n",
      "|    time_elapsed         | 3597       |\n",
      "|    total_timesteps      | 942080     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07206731 |\n",
      "|    clip_fraction        | 0.334      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.334     |\n",
      "|    explained_variance   | 0.211      |\n",
      "|    learning_rate        | 5e-05      |\n",
      "|    loss                 | -0.0277    |\n",
      "|    n_updates            | 1228       |\n",
      "|    policy_gradient_loss | -0.00205   |\n",
      "|    value_loss           | 0.00185    |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=960000, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 360.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 360         |\n",
      "|    mean_reward          | 0           |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 960000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.042736992 |\n",
      "|    clip_fraction        | 0.172       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.241      |\n",
      "|    explained_variance   | 0.644       |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0365     |\n",
      "|    n_updates            | 1248        |\n",
      "|    policy_gradient_loss | -0.0201     |\n",
      "|    value_loss           | 0.00241     |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 9.3        |\n",
      "|    ep_rew_mean          | 0.942      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 261        |\n",
      "|    iterations           | 240        |\n",
      "|    time_elapsed         | 3757       |\n",
      "|    total_timesteps      | 983040     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04830953 |\n",
      "|    clip_fraction        | 0.216      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.25      |\n",
      "|    explained_variance   | 0.22       |\n",
      "|    learning_rate        | 5e-05      |\n",
      "|    loss                 | -0.0236    |\n",
      "|    n_updates            | 1268       |\n",
      "|    policy_gradient_loss | -0.0193    |\n",
      "|    value_loss           | 0.00238    |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 11.5        |\n",
      "|    ep_rew_mean          | 0.91        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 261         |\n",
      "|    iterations           | 250         |\n",
      "|    time_elapsed         | 3910        |\n",
      "|    total_timesteps      | 1024000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.032475546 |\n",
      "|    clip_fraction        | 0.225       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.505      |\n",
      "|    explained_variance   | 0.43        |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0582     |\n",
      "|    n_updates            | 1308        |\n",
      "|    policy_gradient_loss | -0.0168     |\n",
      "|    value_loss           | 0.0193      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 9.4         |\n",
      "|    ep_rew_mean          | 0.941       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 262         |\n",
      "|    iterations           | 260         |\n",
      "|    time_elapsed         | 4061        |\n",
      "|    total_timesteps      | 1064960     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.055284854 |\n",
      "|    clip_fraction        | 0.167       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.259      |\n",
      "|    explained_variance   | 0.569       |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0907     |\n",
      "|    n_updates            | 1348        |\n",
      "|    policy_gradient_loss | -0.0102     |\n",
      "|    value_loss           | 0.000595    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 11.6        |\n",
      "|    ep_rew_mean          | 0.927       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 262         |\n",
      "|    iterations           | 270         |\n",
      "|    time_elapsed         | 4213        |\n",
      "|    total_timesteps      | 1105920     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.051420376 |\n",
      "|    clip_fraction        | 0.185       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.349      |\n",
      "|    explained_variance   | 0.576       |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0488     |\n",
      "|    n_updates            | 1388        |\n",
      "|    policy_gradient_loss | -0.0108     |\n",
      "|    value_loss           | 0.0029      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1120000, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 360.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 360         |\n",
      "|    mean_reward          | 0           |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1120000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.039899867 |\n",
      "|    clip_fraction        | 0.196       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.33       |\n",
      "|    explained_variance   | 0.518       |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | 0.111       |\n",
      "|    n_updates            | 1404        |\n",
      "|    policy_gradient_loss | -0.0253     |\n",
      "|    value_loss           | 0.00072     |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 11.6       |\n",
      "|    ep_rew_mean          | 0.927      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 262        |\n",
      "|    iterations           | 280        |\n",
      "|    time_elapsed         | 4371       |\n",
      "|    total_timesteps      | 1146880    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04791307 |\n",
      "|    clip_fraction        | 0.239      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.288     |\n",
      "|    explained_variance   | 0.331      |\n",
      "|    learning_rate        | 5e-05      |\n",
      "|    loss                 | -0.0663    |\n",
      "|    n_updates            | 1428       |\n",
      "|    policy_gradient_loss | -0.0227    |\n",
      "|    value_loss           | 0.00376    |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 10.3       |\n",
      "|    ep_rew_mean          | 0.935      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 262        |\n",
      "|    iterations           | 290        |\n",
      "|    time_elapsed         | 4521       |\n",
      "|    total_timesteps      | 1187840    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03928318 |\n",
      "|    clip_fraction        | 0.152      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.327     |\n",
      "|    explained_variance   | 0.439      |\n",
      "|    learning_rate        | 5e-05      |\n",
      "|    loss                 | -0.0215    |\n",
      "|    n_updates            | 1468       |\n",
      "|    policy_gradient_loss | -0.0117    |\n",
      "|    value_loss           | 0.0131     |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 11.5       |\n",
      "|    ep_rew_mean          | 0.909      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 262        |\n",
      "|    iterations           | 300        |\n",
      "|    time_elapsed         | 4674       |\n",
      "|    total_timesteps      | 1228800    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03453715 |\n",
      "|    clip_fraction        | 0.25       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.429     |\n",
      "|    explained_variance   | 0.231      |\n",
      "|    learning_rate        | 5e-05      |\n",
      "|    loss                 | 0.223      |\n",
      "|    n_updates            | 1508       |\n",
      "|    policy_gradient_loss | -0.0188    |\n",
      "|    value_loss           | 0.0028     |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 8.23       |\n",
      "|    ep_rew_mean          | 0.862      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 263        |\n",
      "|    iterations           | 310        |\n",
      "|    time_elapsed         | 4824       |\n",
      "|    total_timesteps      | 1269760    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04977785 |\n",
      "|    clip_fraction        | 0.18       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.343     |\n",
      "|    explained_variance   | 0.292      |\n",
      "|    learning_rate        | 5e-05      |\n",
      "|    loss                 | -0.0204    |\n",
      "|    n_updates            | 1548       |\n",
      "|    policy_gradient_loss | -0.00239   |\n",
      "|    value_loss           | 0.00694    |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=1280000, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 360.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 360        |\n",
      "|    mean_reward          | 0          |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1280000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09286179 |\n",
      "|    clip_fraction        | 0.223      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.406     |\n",
      "|    explained_variance   | 0.327      |\n",
      "|    learning_rate        | 5e-05      |\n",
      "|    loss                 | -0.00954   |\n",
      "|    n_updates            | 1560       |\n",
      "|    policy_gradient_loss | -0.03      |\n",
      "|    value_loss           | 0.00265    |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 9.34        |\n",
      "|    ep_rew_mean          | 0.942       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 262         |\n",
      "|    iterations           | 320         |\n",
      "|    time_elapsed         | 4984        |\n",
      "|    total_timesteps      | 1310720     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025258139 |\n",
      "|    clip_fraction        | 0.15        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.316      |\n",
      "|    explained_variance   | 0.43        |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0394     |\n",
      "|    n_updates            | 1588        |\n",
      "|    policy_gradient_loss | -0.0179     |\n",
      "|    value_loss           | 0.00193     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 8.97        |\n",
      "|    ep_rew_mean          | 0.944       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 263         |\n",
      "|    iterations           | 330         |\n",
      "|    time_elapsed         | 5135        |\n",
      "|    total_timesteps      | 1351680     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.048304394 |\n",
      "|    clip_fraction        | 0.213       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.276      |\n",
      "|    explained_variance   | 0.533       |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0459     |\n",
      "|    n_updates            | 1628        |\n",
      "|    policy_gradient_loss | -0.0207     |\n",
      "|    value_loss           | 0.000604    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 8.67        |\n",
      "|    ep_rew_mean          | 0.946       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 263         |\n",
      "|    iterations           | 340         |\n",
      "|    time_elapsed         | 5288        |\n",
      "|    total_timesteps      | 1392640     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022712782 |\n",
      "|    clip_fraction        | 0.174       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.327      |\n",
      "|    explained_variance   | 0.517       |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0676     |\n",
      "|    n_updates            | 1668        |\n",
      "|    policy_gradient_loss | -0.0232     |\n",
      "|    value_loss           | 0.000382    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 9.19        |\n",
      "|    ep_rew_mean          | 0.943       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 263         |\n",
      "|    iterations           | 350         |\n",
      "|    time_elapsed         | 5438        |\n",
      "|    total_timesteps      | 1433600     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.031717613 |\n",
      "|    clip_fraction        | 0.138       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.272      |\n",
      "|    explained_variance   | 0.39        |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0509     |\n",
      "|    n_updates            | 1708        |\n",
      "|    policy_gradient_loss | -0.0109     |\n",
      "|    value_loss           | 0.000485    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1440000, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 360.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 360         |\n",
      "|    mean_reward          | 0           |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1440000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030493394 |\n",
      "|    clip_fraction        | 0.156       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.267      |\n",
      "|    explained_variance   | 0.63        |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0443     |\n",
      "|    n_updates            | 1716        |\n",
      "|    policy_gradient_loss | -0.00853    |\n",
      "|    value_loss           | 0.00496     |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 8.83         |\n",
      "|    ep_rew_mean          | 0.945        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 263          |\n",
      "|    iterations           | 360          |\n",
      "|    time_elapsed         | 5598         |\n",
      "|    total_timesteps      | 1474560      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0144636445 |\n",
      "|    clip_fraction        | 0.0626       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0689      |\n",
      "|    explained_variance   | 0.388        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | -0.0292      |\n",
      "|    n_updates            | 1748         |\n",
      "|    policy_gradient_loss | -0.000578    |\n",
      "|    value_loss           | 0.000197     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 8.93        |\n",
      "|    ep_rew_mean          | 0.944       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 263         |\n",
      "|    iterations           | 370         |\n",
      "|    time_elapsed         | 5749        |\n",
      "|    total_timesteps      | 1515520     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030463096 |\n",
      "|    clip_fraction        | 0.144       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.278      |\n",
      "|    explained_variance   | 0.389       |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0435     |\n",
      "|    n_updates            | 1788        |\n",
      "|    policy_gradient_loss | -0.0186     |\n",
      "|    value_loss           | 0.00114     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 9.54        |\n",
      "|    ep_rew_mean          | 0.94        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 263         |\n",
      "|    iterations           | 380         |\n",
      "|    time_elapsed         | 5900        |\n",
      "|    total_timesteps      | 1556480     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026717607 |\n",
      "|    clip_fraction        | 0.146       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.227      |\n",
      "|    explained_variance   | 0.557       |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | 0.0632      |\n",
      "|    n_updates            | 1828        |\n",
      "|    policy_gradient_loss | -0.00546    |\n",
      "|    value_loss           | 0.00159     |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 15.7       |\n",
      "|    ep_rew_mean          | 0.902      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 263        |\n",
      "|    iterations           | 390        |\n",
      "|    time_elapsed         | 6053       |\n",
      "|    total_timesteps      | 1597440    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09981101 |\n",
      "|    clip_fraction        | 0.355      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.476     |\n",
      "|    explained_variance   | -0.162     |\n",
      "|    learning_rate        | 5e-05      |\n",
      "|    loss                 | 0.0197     |\n",
      "|    n_updates            | 1868       |\n",
      "|    policy_gradient_loss | 0.00167    |\n",
      "|    value_loss           | 0.00103    |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=1600000, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 360.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 360         |\n",
      "|    mean_reward          | 0           |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1600000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.050072547 |\n",
      "|    clip_fraction        | 0.321       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.487      |\n",
      "|    explained_variance   | 0.0324      |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0731     |\n",
      "|    n_updates            | 1872        |\n",
      "|    policy_gradient_loss | -0.0147     |\n",
      "|    value_loss           | 0.0027      |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 13.2       |\n",
      "|    ep_rew_mean          | 0.918      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 263        |\n",
      "|    iterations           | 400        |\n",
      "|    time_elapsed         | 6211       |\n",
      "|    total_timesteps      | 1638400    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03408032 |\n",
      "|    clip_fraction        | 0.299      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.539     |\n",
      "|    explained_variance   | 0.0348     |\n",
      "|    learning_rate        | 5e-05      |\n",
      "|    loss                 | 0.179      |\n",
      "|    n_updates            | 1908       |\n",
      "|    policy_gradient_loss | -0.0168    |\n",
      "|    value_loss           | 0.00314    |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 9.7        |\n",
      "|    ep_rew_mean          | 0.939      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 263        |\n",
      "|    iterations           | 410        |\n",
      "|    time_elapsed         | 6362       |\n",
      "|    total_timesteps      | 1679360    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06187509 |\n",
      "|    clip_fraction        | 0.206      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.25      |\n",
      "|    explained_variance   | 0.613      |\n",
      "|    learning_rate        | 5e-05      |\n",
      "|    loss                 | -0.0883    |\n",
      "|    n_updates            | 1948       |\n",
      "|    policy_gradient_loss | -0.0159    |\n",
      "|    value_loss           | 0.00122    |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 7.71       |\n",
      "|    ep_rew_mean          | 0.843      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 264        |\n",
      "|    iterations           | 420        |\n",
      "|    time_elapsed         | 6514       |\n",
      "|    total_timesteps      | 1720320    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02172234 |\n",
      "|    clip_fraction        | 0.061      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.0745    |\n",
      "|    explained_variance   | 0.171      |\n",
      "|    learning_rate        | 5e-05      |\n",
      "|    loss                 | -0.000419  |\n",
      "|    n_updates            | 1988       |\n",
      "|    policy_gradient_loss | -0.0123    |\n",
      "|    value_loss           | 0.0122     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=1760000, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 360.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 360        |\n",
      "|    mean_reward          | 0          |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1760000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04460989 |\n",
      "|    clip_fraction        | 0.128      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.183     |\n",
      "|    explained_variance   | 0.437      |\n",
      "|    learning_rate        | 5e-05      |\n",
      "|    loss                 | -0.0464    |\n",
      "|    n_updates            | 2028       |\n",
      "|    policy_gradient_loss | 0.0168     |\n",
      "|    value_loss           | 0.000815   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 9.71     |\n",
      "|    ep_rew_mean     | 0.884    |\n",
      "| time/              |          |\n",
      "|    fps             | 263      |\n",
      "|    iterations      | 430      |\n",
      "|    time_elapsed    | 6674     |\n",
      "|    total_timesteps | 1761280  |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 10.7        |\n",
      "|    ep_rew_mean          | 0.914       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 264         |\n",
      "|    iterations           | 440         |\n",
      "|    time_elapsed         | 6822        |\n",
      "|    total_timesteps      | 1802240     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.033772837 |\n",
      "|    clip_fraction        | 0.171       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.329      |\n",
      "|    explained_variance   | 0.674       |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0593     |\n",
      "|    n_updates            | 2068        |\n",
      "|    policy_gradient_loss | -0.00796    |\n",
      "|    value_loss           | 0.00204     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 8.99        |\n",
      "|    ep_rew_mean          | 0.944       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 264         |\n",
      "|    iterations           | 450         |\n",
      "|    time_elapsed         | 6971        |\n",
      "|    total_timesteps      | 1843200     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.035755254 |\n",
      "|    clip_fraction        | 0.12        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.0843     |\n",
      "|    explained_variance   | 0.621       |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0391     |\n",
      "|    n_updates            | 2108        |\n",
      "|    policy_gradient_loss | -0.0067     |\n",
      "|    value_loss           | 0.000156    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 13.3        |\n",
      "|    ep_rew_mean          | 0.909       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 264         |\n",
      "|    iterations           | 460         |\n",
      "|    time_elapsed         | 7121        |\n",
      "|    total_timesteps      | 1884160     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.038594373 |\n",
      "|    clip_fraction        | 0.206       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.58       |\n",
      "|    explained_variance   | 0.206       |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0389     |\n",
      "|    n_updates            | 2148        |\n",
      "|    policy_gradient_loss | -0.0226     |\n",
      "|    value_loss           | 0.0143      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1920000, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 360.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 360         |\n",
      "|    mean_reward          | 0           |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1920000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.036542878 |\n",
      "|    clip_fraction        | 0.234       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.269      |\n",
      "|    explained_variance   | 0.708       |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0749     |\n",
      "|    n_updates            | 2184        |\n",
      "|    policy_gradient_loss | -0.0303     |\n",
      "|    value_loss           | 0.000632    |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 9.21       |\n",
      "|    ep_rew_mean          | 0.942      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 264        |\n",
      "|    iterations           | 470        |\n",
      "|    time_elapsed         | 7276       |\n",
      "|    total_timesteps      | 1925120    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07991801 |\n",
      "|    clip_fraction        | 0.187      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.136     |\n",
      "|    explained_variance   | 0.692      |\n",
      "|    learning_rate        | 5e-05      |\n",
      "|    loss                 | -0.0601    |\n",
      "|    n_updates            | 2188       |\n",
      "|    policy_gradient_loss | -0.0112    |\n",
      "|    value_loss           | 0.000245   |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 9.75        |\n",
      "|    ep_rew_mean          | 0.939       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 264         |\n",
      "|    iterations           | 480         |\n",
      "|    time_elapsed         | 7424        |\n",
      "|    total_timesteps      | 1966080     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.046969395 |\n",
      "|    clip_fraction        | 0.157       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.385      |\n",
      "|    explained_variance   | -0.944      |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.106      |\n",
      "|    n_updates            | 2228        |\n",
      "|    policy_gradient_loss | -0.0108     |\n",
      "|    value_loss           | 0.00278     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x7f0d485536d0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of timesteps to add\n",
    "total_timesteps = 2000000\n",
    "log_interval = 10\n",
    "\n",
    "# resume training using same tensorboard run without resetting the timesteps\n",
    "model.learn(total_timesteps=total_timesteps,\n",
    "            log_interval=log_interval,\n",
    "            tb_log_name=tb_log_name,\n",
    "            reset_num_timesteps=True,\n",
    "            callback=eval_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5e9ae39a",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Save the agent\n",
    "model.save(tb_log_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "39db72f4-e738-4aa8-93b4-8e26a152f934",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO.load(path=tb_log_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6647e7",
   "metadata": {},
   "source": [
    "Evaluate training on second tast:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f54500cb",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean reward: 0.5703125089406967 +/- 0.47\n"
     ]
    }
   ],
   "source": [
    "# We create a separate environment for evaluation\n",
    "eval_env = ImgRGBImgPartialObsWrapper(gym.make(env_id_2))\n",
    "eval_env.seed(10000+randint(0, 10))\n",
    "eval_env.reset()\n",
    "# Random Agent, before training\n",
    "mean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=20)\n",
    "print(f'Mean reward: {mean_reward} +/- {std_reward:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9ee8970d-8749-4bec-a90e-b8f187cf113e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reward: 0.9375\n",
      "Total length: 10\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<video alt=\"test\" autoplay \n",
       "                loop controls style=\"height: 400px;\">\n",
       "                <source src=\"data:video/mp4;base64,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\" type=\"video/mp4\" />\n",
       "             </video>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_env = gen_wrapped_env_cnn(env_id_2)\n",
    "test_env.seed(10000+randint(0, 10))\n",
    "observation = test_env.reset()\n",
    "\n",
    "done = False\n",
    "episode_reward = 0\n",
    "episode_length = 0\n",
    "\n",
    "while not done:\n",
    "  #test_env.render()\n",
    "  action, states = model.predict(observation, deterministic=False)\n",
    "  observation, reward, done, info = test_env.step(action)\n",
    "  episode_reward += reward\n",
    "  episode_length += 1\n",
    "\n",
    "print('Total reward:', episode_reward)\n",
    "print('Total length:', episode_length)\n",
    "\n",
    "test_env.close()\n",
    "show_video()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58679a46",
   "metadata": {},
   "source": [
    "## Re-evaluate first task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4176a50f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/inigo/stable-baselines3/stable_baselines3/common/evaluation.py:69: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  UserWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean reward: 0.0 +/- 0.00\n"
     ]
    }
   ],
   "source": [
    "# We create a separate environment for evaluation\n",
    "eval_env = ImgRGBImgPartialObsWrapper(gym.make(env_id_1))\n",
    "eval_env.seed(10000+randint(0, 10))\n",
    "eval_env.reset()\n",
    "# Random Agent, before training\n",
    "mean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=20)\n",
    "print(f'Mean reward: {mean_reward} +/- {std_reward:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1f1bb5-2a8a-4545-90a0-9d5a78f0597b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "test_minigrid_sb3_curriculum.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "51a1b245ec7fc72d2061587b663ba1f584a95fe24cf1a61a943e144e8f966db4"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
